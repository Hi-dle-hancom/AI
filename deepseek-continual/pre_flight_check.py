#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ì‹¤í–‰ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Critical Issues í•´ê²° ìƒíƒœ ë° í™˜ê²½ ì¤€ë¹„ ìƒíƒœ ê²€ì¦

ì‹¤í–‰ ë°©ë²•:
    python pre_flight_check.py
"""

import os
import sys
import yaml
import subprocess
import torch
from pathlib import Path

def check_gpu_environment():
    """GPU í™˜ê²½ ì²´í¬"""
    print("ğŸ” GPU í™˜ê²½ ì²´í¬")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # GPU ì •ë³´
    props = torch.cuda.get_device_properties(0)
    total_mem_gb = props.total_memory / (1024**3)
    
    print(f"âœ… GPU: {props.name}")
    print(f"âœ… CUDA ë²„ì „: {torch.version.cuda}")
    print(f"âœ… PyTorch ë²„ì „: {torch.__version__}")
    print(f"âœ… ì´ ë©”ëª¨ë¦¬: {total_mem_gb:.1f}GB")
    
    # A10G í™˜ê²½ ì²´í¬
    if "A10G" in props.name or total_mem_gb > 20:
        print("âœ… A10G 22GB í™˜ê²½ ê°ì§€ë¨")
    else:
        print(f"âš ï¸  A10Gê°€ ì•„ë‹Œ GPU ê°ì§€: {props.name} ({total_mem_gb:.1f}GB)")
        print("ğŸ’¡ ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ 1ë¡œ, max_lengthë¥¼ 256ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
    
    return True

def check_dependencies():
    """ì˜ì¡´ì„± ì²´í¬"""
    print("\nğŸ” ì˜ì¡´ì„± ì²´í¬")
    
    required_packages = {
        'torch': '2.0.0',
        'transformers': '4.30.0',
        'datasets': '2.10.0',
        'peft': '0.4.0',
        'accelerate': '0.20.0',
        'bitsandbytes': '0.39.0'
    }
    
    missing_packages = []
    
    for package, min_version in required_packages.items():
        try:
            module = __import__(package)
            version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {package}: {version}")
        except ImportError:
            print(f"âŒ {package}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nğŸ’¡ ì„¤ì¹˜ ëª…ë ¹ì–´:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    return True

def check_config_file():
    """config.yaml íŒŒì¼ ì²´í¬"""
    print("\nğŸ” config.yaml ì²´í¬")
    
    config_path = "config.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ config.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        return False
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # Critical Issues í•´ê²° ìƒíƒœ ì²´í¬
        batch_size = config.get('batch_size', 1)
        max_length = config.get('max_length', 1024)
        gpu_memory_fraction = config.get('gpu_memory_fraction', 0.8)
        
        print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"ğŸ“Š ìµœëŒ€ ê¸¸ì´: {max_length}")
        print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ì œí•œ: {gpu_memory_fraction*100:.0f}%")
        
        # Critical Issue #3 ì²´í¬
        if batch_size < 2:
            print("âš ï¸  ê²½ê³ : ë°°ì¹˜ í¬ê¸°ê°€ 2 ë¯¸ë§Œì…ë‹ˆë‹¤. BatchNorm/LayerNorm ë¶ˆì•ˆì • ê°€ëŠ¥ì„±")
            print("ğŸ’¡ ì¶”ì²œ: batch_sizeë¥¼ 2 ì´ìƒìœ¼ë¡œ ì„¤ì •")
        else:
            print("âœ… ë°°ì¹˜ í¬ê¸° ì„¤ì • ì–‘í˜¸")
        
        if max_length > 512:
            print("âš ï¸  ê²½ê³ : max_lengthê°€ 512ë³´ë‹¤ í½ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±")
            print("ğŸ’¡ ì¶”ì²œ: max_lengthë¥¼ 512 ì´í•˜ë¡œ ì„¤ì •")
        else:
            print("âœ… ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì • ì–‘í˜¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ config.yaml íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return False

def check_data_files():
    """ë°ì´í„° íŒŒì¼ ì²´í¬"""
    print("\nğŸ” ë°ì´í„° íŒŒì¼ ì²´í¬")
    
    data_paths = [
        os.path.expanduser("~/deepseek-continual/data/train.jsonl"),
        "data/train.jsonl",
        "/home/ubuntu/deepseek-continual/data/train.jsonl"
    ]
    
    data_found = False
    for data_path in data_paths:
        if os.path.exists(data_path):
            file_size_mb = os.path.getsize(data_path) / (1024 * 1024)
            print(f"âœ… ë°ì´í„° íŒŒì¼ ë°œê²¬: {data_path} ({file_size_mb:.1f}MB)")
            
            # ìƒ˜í”Œ ë¼ì¸ ìˆ˜ ì²´í¬
            try:
                with open(data_path, 'r', encoding='utf-8') as f:
                    line_count = sum(1 for _ in f)
                print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {line_count:,}ê°œ")
                data_found = True
                break
            except Exception as e:
                print(f"âš ï¸  íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
    
    if not data_found:
        print("âŒ í•™ìŠµ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒ ê²½ë¡œ ì¤‘ í•˜ë‚˜ì— train.jsonl íŒŒì¼ì„ ë°°ì¹˜í•˜ì„¸ìš”:")
        for path in data_paths:
            print(f"  - {path}")
        return False
    
    return True

def check_output_directories():
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ì²´í¬"""
    print("\nğŸ” ì¶œë ¥ ë””ë ‰í† ë¦¬ ì²´í¬")
    
    output_dirs = [
        "checkpoints",
        "../logs",
        "../models"
    ]
    
    for dir_path in output_dirs:
        if not os.path.exists(dir_path):
            try:
                os.makedirs(dir_path, exist_ok=True)
                print(f"âœ… ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")
            except Exception as e:
                print(f"âŒ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {dir_path} - {e}")
                return False
        else:
            print(f"âœ… ë””ë ‰í† ë¦¬ ì¡´ì¬: {dir_path}")
    
    return True

def check_memory_settings():
    """ë©”ëª¨ë¦¬ ì„¤ì • ì²´í¬"""
    print("\nğŸ” ë©”ëª¨ë¦¬ ì„¤ì • ì²´í¬")
    
    # CUDA ë©”ëª¨ë¦¬ ì„¤ì • í™•ì¸
    cuda_alloc_conf = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', '')
    
    if 'expandable_segments:True' in cuda_alloc_conf:
        print("âœ… CUDA ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ ì„¤ì • í™œì„±í™”ë¨")
    else:
        print("âš ï¸  CUDA ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í™˜ê²½ë³€ìˆ˜ ì„¤ì •: export PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True,max_split_size_mb:128'")
    
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
    if torch.cuda.is_available():
        allocated_gb = torch.cuda.memory_allocated(0) / (1024**3)
        reserved_gb = torch.cuda.memory_reserved(0) / (1024**3)
        
        if allocated_gb > 1.0:
            print(f"âš ï¸  GPU ë©”ëª¨ë¦¬ê°€ ì´ë¯¸ {allocated_gb:.1f}GB ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            print("âœ… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì–‘í˜¸")
    
    return True

def run_all_checks():
    """ëª¨ë“  ì²´í¬ ì‹¤í–‰"""
    print("ğŸš€ Continual Learning ì‹¤í–‰ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸")
    print("=" * 50)
    
    checks = [
        ("GPU í™˜ê²½", check_gpu_environment),
        ("ì˜ì¡´ì„±", check_dependencies),
        ("Config íŒŒì¼", check_config_file),
        ("ë°ì´í„° íŒŒì¼", check_data_files),
        ("ì¶œë ¥ ë””ë ‰í† ë¦¬", check_output_directories),
        ("ë©”ëª¨ë¦¬ ì„¤ì •", check_memory_settings)
    ]
    
    passed_checks = 0
    total_checks = len(checks)
    
    for check_name, check_func in checks:
        try:
            if check_func():
                passed_checks += 1
        except Exception as e:
            print(f"âŒ {check_name} ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š ì²´í¬ ê²°ê³¼: {passed_checks}/{total_checks} í†µê³¼")
    
    if passed_checks == total_checks:
        print("ğŸ‰ ëª¨ë“  ì²´í¬ í†µê³¼! í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("\nğŸ’¡ ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ:")
        print("1. ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸: python test_small_batch.py --samples 100")
        print("2. ì¤‘ê°„ í…ŒìŠ¤íŠ¸: python test_small_batch.py --samples 1000")
        print("3. ì „ì²´ í•™ìŠµ: ./cc_run_training.sh comment")
        return True
    else:
        print("âŒ ì¼ë¶€ ì²´í¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë¬¸ì œì ë“¤ì„ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return False

def main():
    success = run_all_checks()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
