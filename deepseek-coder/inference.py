#!/usr/bin/env python3
"""
DeepSeek-Coder ì‹¤ì‹œê°„ ì½”ë“œ ì–´ì‹œìŠ¤íŠ¸ ì¸í¼ëŸ°ìŠ¤ ìŠ¤í¬ë¦½íŠ¸ (T4 í™˜ê²½ ìµœì í™”)
ì§€ì› ê¸°ëŠ¥:
- ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ í”„ëŸ¼í”„íŠ¸ ì‘ë‹µ
- ì˜¤ë¥˜ ìˆ˜ì • ë° ì‘ì„± ì§€ì›
- ì½”ë“œ ìë™ì™„ì„±
- T4 GPU ì „ìš© ìµœì í™”
"""

import os
import sys
import json
import argparse
import torch
import readline
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ T4 í™˜ê²½ ìµœì í™” ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_CONFIG = {
    "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",  # ê¸°ë³¸ ëª¨ë¸
    "model_path": "../models/prompt-finetuned/final_model",  # train.py [2ì°¨] í•™ìŠµ ì¶œë ¥ ê²½ë¡œ
    "device": "cuda" if torch.cuda.is_available() else "cpu", 
    "dtype": torch.float16,  # T4ì—ì„œ bfloat16 ëŒ€ì‹  float16 ì‚¬ìš©
    "max_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.95,
    "load_in_4bit": True,  # T4 ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ 4-bit ì–‘ìí™”
}

def check_gpu_compatibility():
    """GPU í˜¸í™˜ì„± í™•ì¸"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        compute_capability = torch.cuda.get_device_capability(0)
        print(f"ğŸ–¥ï¸ GPU: {gpu_name}")
        print(f"ğŸ”§ Compute Capability: {compute_capability[0]}.{compute_capability[1]}")
        
        # bfloat16 ì§€ì› í™•ì¸
        if torch.cuda.is_bf16_supported():
            print("âœ… bfloat16 ì§€ì›ë¨")
        else:
            print("âš ï¸ bfloat16 ë¯¸ì§€ì› - float16 ì‚¬ìš©")
            
        return True
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œ")
        return False

class CodeAssistant:
    """ì½”ë“œ ì–´ì‹œìŠ¤íŠ¸ ì¶”ë¡  í´ë˜ìŠ¤ (T4 ìµœì í™”)"""
    
    def __init__(self, config=None, mode="code-gen"):
        self.config = config or MODEL_CONFIG
        self.mode = mode
        
        # GPU í˜¸í™˜ì„± í™•ì¸
        self.cuda_available = check_gpu_compatibility()
        
        # T4 í™˜ê²½ì—ì„œ dtype ì¡°ì •
        if self.cuda_available:
            # T4ì—ì„œëŠ” bfloat16 ëŒ€ì‹  float16 ì‚¬ìš©
            self.config["dtype"] = torch.float16
        else:
            self.config["dtype"] = torch.float32
            self.config["device"] = "cpu"
        
        # ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ëª¨ë¸ ì„ íƒ
        model_path = self._get_model_path()
        self.config["model_path"] = model_path
        
        self.model = None
        self.tokenizer = None
        self.load_model()
        
    def _get_model_path(self):
        """ëª¨ë“œì— ë”°ë¥¸ ëª¨ë¸ ê²½ë¡œ ê²°ì •"""
        if self.mode == "autocomplete":
            model_path = "../models/autocomplete-finetuned/final_model"
            if not os.path.exists(model_path):
                print(f"\nâš ï¸ [ì œ1ì°¨] ìë™ì™„ì„± ì „ìš© ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                model_path = self.config["model_path"]
        elif self.mode == "comment":
            model_path = "../models/comment-finetuned/final_model"
            if not os.path.exists(model_path):
                print(f"\nâš ï¸ [ì œ3ì°¨] ì£¼ì„ ê¸°ë°˜ ì½”ë“œ ìƒì„± ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                model_path = self.config["model_path"]
        elif self.mode == "error_fix":
            model_path = "../models/error-fix-finetuned/final_model"
            if not os.path.exists(model_path):
                print(f"\nâš ï¸ [ì œ4ì°¨] ì˜¤ë¥˜ ì½”ë“œ ìˆ˜ì • ì „ìš© ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                model_path = self.config["model_path"]
        else:  # prompt ëª¨ë“œ(ê¸°ë³¸ê°’)
            model_path = self.config["model_path"]  # prompt ëª¨ë¸ì´ ê¸°ë³¸ê°’
        
        return model_path
        
    def load_model(self):
        """T4 í™˜ê²½ ìµœì í™”ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print(f"\nğŸ“‚ ëª¨ë¸ ë¡œë”© ì‹œì‘ (T4 ìµœì í™”)")
        model_path = self.config.get('model_path')
        model_id = self.config.get('model_id')
        
        # ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¨¼ì € ì‹œë„
        try_path = model_path if os.path.exists(model_path) else model_id
        print(f"ğŸ” ëª¨ë¸ ê²½ë¡œ: {try_path}")
        print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.config['device']}")
        print(f"ğŸ”¢ ë°ì´í„° íƒ€ì…: {self.config['dtype']}")
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            self.tokenizer = AutoTokenizer.from_pretrained(
                try_path, 
                trust_remote_code=True
            )
            
            # T4 í™˜ê²½ ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©
            if self.cuda_available and self.config.get("load_in_4bit", False):
                # 4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
                from transformers import BitsAndBytesConfig
                
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,  # T4 í˜¸í™˜
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    try_path,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16
                )
                print("âœ… 4-bit ì–‘ìí™” ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                # ì¼ë°˜ ë¡œë”©
                self.model = AutoModelForCausalLM.from_pretrained(
                    try_path,
                    torch_dtype=self.config['dtype'],
                    device_map="auto" if self.config['device'] == "cuda" else None,
                    trust_remote_code=True
                )
                
                # CPUì˜ ê²½ìš° ëª…ì‹œì  ì´ë™
                if self.config['device'] == "cpu":
                    self.model = self.model.to("cpu")
                    
            self.model.eval()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if self.cuda_available:
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f}GB / {memory_total:.2f}GB")
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ í•´ê²° ë°©ì•ˆ:")
            print("   1. config.yamlì—ì„œ bnb_4bit_compute_dtype: float16ìœ¼ë¡œ ì„¤ì •")
            print("   2. torch_dtype: float16ìœ¼ë¡œ ì„¤ì •")
            print("   3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ per_device_train_batch_size ì¤„ì´ê¸°")
            sys.exit(1)
    
    def get_int_config(self, key, default=0):
        """configì—ì„œ ê°’ì„ ê°€ì ¸ì™€ intë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        value = self.config.get(key, default)
        if isinstance(value, str):
            try:
                value = int(value)
            except ValueError:
                print(f"âš ï¸ ê²½ê³ : {key} ê°’ '{value}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
                value = default
        return value
    
    def get_float_config(self, key, default=0.0):
        """configì—ì„œ ê°’ì„ ê°€ì ¸ì™€ floatë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        value = self.config.get(key, default)
        if isinstance(value, str):
            try:
                value = float(value)
            except ValueError:
                print(f"âš ï¸ ê²½ê³ : {key} ê°’ '{value}'ì„ ì‹¤ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
                value = default
        return value
    
    def build_prompt(self, user_input, mode="prompt"):
        """ChatML í˜•ì‹ìœ¼ë¡œ í”„ëŸ¼í”„íŠ¸ êµ¬ì„± (ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡° ì§€ì›)"""
        system_msg = "You are an expert coding assistant."
        
        # ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ ì²˜ë¦¬
        if isinstance(user_input, dict):
            # 1. ì±„íŒ… í˜•ì‹ (messages êµ¬ì¡°)
            if "messages" in user_input:
                messages = user_input["messages"]
                chat_text = ""
                
                # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
                chat_text += f"<|im_start|>system\n{system_msg}<|im_end|>\n"
                
                # ëª¨ë“  ë©”ì‹œì§€ë¥¼ ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    chat_text += f"<|im_start|>{role}\n{content}<|im_end|>\n"
                
                # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì‹œì‘ íƒœê·¸ ì¶”ê°€
                chat_text += "<|im_start|>assistant\n"
                return chat_text
            
            # 2. ì£¼ì„ í‚¤ì›Œë“œ ê¸°ë°˜ ì½”ë“œ ìƒì„± (prompt-completion)
            elif "prompt" in user_input:
                prompt = user_input["prompt"]
                
                # FIM í˜•ì‹ íŠ¹ìˆ˜ ì²˜ë¦¬
                if "<ï½œfim beginï½œ>" in prompt and "<ï½œfim holeï½œ>" in prompt and "<ï½œfim endï½œ>" in prompt:
                    # fim beginê³¼ fim hole ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (prefix)
                    prefix = prompt.split("<ï½œfim holeï½œ>")[0].replace("<ï½œfim beginï½œ>", "")
                    # fim holeê³¼ fim end ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (suffix)
                    suffix = prompt.split("<ï½œfim holeï½œ>")[1].split("<ï½œfim endï½œ>")[0]
                    
                    # FIM ìŠ¤í˜ì…œ í† í° í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
                    return f"<|fim_prefix|>{prefix}<|fim_suffix|>{suffix}<|fim_middle|"
                else:
                    # ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
                    return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            
            # 3. ì—ëŸ¬ ì„¤ëª… í˜•ì‹
            elif "error_context" in user_input and "explanation" in user_input.get("error_context", {}):
                error_context = user_input["error_context"]
                error_log = error_context.get("error_log", "")
                code_snippet = error_context.get("code_snippet", "")
                language = error_context.get("language", "")
                
                user_text = f"ë‹¤ìŒ {language} ì½”ë“œì˜ ì—ëŸ¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”:\n\nì—ëŸ¬ ë¡œê·¸:\n{error_log}\n\nì½”ë“œ:\n{code_snippet}"
                return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
            
            # 4. ì—ëŸ¬ ìˆ˜ì • í˜•ì‹
            elif "error_context" in user_input and "buggy_code_snippet" in user_input:
                error_context = user_input["error_context"]
                buggy_code = user_input["buggy_code_snippet"]
                error_log = error_context.get("error_log", "")
                language = error_context.get("language", "")
                
                user_text = f"ë‹¤ìŒ {language} ì½”ë“œì˜ ì—ëŸ¬ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n\nì—ëŸ¬ ë¡œê·¸:\n{error_log}\n\nì½”ë“œ:\n{buggy_code}"
                return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
            
            # 5. ê¸°ì¡´ instruction/input í˜•ì‹
            elif "instruction" in user_input:
                instruction = user_input.get("instruction", "").strip()
                input_text = user_input.get("input", "").strip()
                
                # instructionê³¼ inputì„ í•©ì³ì„œ user ë©”ì‹œì§€ë¡œ êµ¬ì„±
                if input_text:
                    user_text = f"Instruction: {instruction}\nInput: {input_text}"
                else:
                    user_text = f"Instruction: {instruction}"
                
                # ChatML í˜•ì‹ í”„ëŸ¼í”„íŠ¸
                if mode == "error_fix":
                    return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\nPlease fix the following code:\n\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
                elif mode == "autocomplete":
                    return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\nComplete the following code:\n\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
                elif mode == "comment":
                    return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\nGenerate code based on this comment:\n\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
                else:  # prompt
                    return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
            
            # ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥
            else:
                user_text = json.dumps(user_input, ensure_ascii=False)
        else:
            # ê¸°ì¡´ ë¬¸ìì—´ ì…ë ¥ ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
            user_text = str(user_input)
        
        # ê¸°ë³¸ ChatML í˜•ì‹ í”„ëŸ¼í”„íŠ¸
        if mode == "error_fix":
            return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\nPlease fix the following code:\n\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
        elif mode == "autocomplete":
            return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\nComplete the following code:\n\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
        elif mode == "comment":
            return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\nGenerate code based on this comment:\n\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
        else:  # prompt
            return f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n"
    
    def generate(self, user_input, mode="prompt"):
        """T4 ìµœì í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
        # ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ í™•ì¸
        is_fim_format = False
        if isinstance(user_input, dict) and "prompt" in user_input:
            prompt_str = user_input["prompt"]
            if "<ï½œfim beginï½œ>" in prompt_str and "<ï½œfim holeï½œ>" in prompt_str and "<ï½œfim endï½œ>" in prompt_str:
                is_fim_format = True
                
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.build_prompt(user_input, mode)
        
        # T4 ë©”ëª¨ë¦¬ ì œí•œì„ ê³ ë ¤í•œ í† í° ê¸¸ì´ ì¡°ì •
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        
        if self.config['device'] == "cuda":
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        # ëª¨ë“œë³„ ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
        if is_fim_format:
            # FIM ìš© íŠ¹ìˆ˜ ì„¤ì •
            max_new_tokens = min(512, self.get_int_config('max_tokens', 512))
            temp = 0.2  # ë” ê²°ì •ì ìœ¼ë¡œ
        elif mode == "autocomplete":
            max_new_tokens = min(128, self.get_int_config('max_tokens', 512))  # ìë™ì™„ì„±ì€ ì§§ê²Œ
            temp = 0.3  # ë” ê²°ì •ì ìœ¼ë¡œ
        elif mode == "comment":
            max_new_tokens = min(256, self.get_int_config('max_tokens', 512))
            temp = self.get_float_config('temperature', 0.6)
        else:  # prompt, error_fix
            max_new_tokens = self.get_int_config('max_tokens', 512)
            temp = self.get_float_config('temperature', 0.6)
            
        with torch.no_grad():
            # T4 ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ìƒì„± ì„¤ì •
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=(mode != "autocomplete" and not is_fim_format),
                top_p=self.get_float_config('top_p', 0.95),
                temperature=temp,
                top_k=self.get_int_config('top_k', 50),
                repetition_penalty=self.get_float_config('repetition_penalty', 1.1),
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True,  # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            )
        
        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì´í›„ ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        input_length = inputs['input_ids'].shape[1]
        generated = self.tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ë°˜í™˜
        return self._clean_output(generated)
    
    def _clean_output(self, text):
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ChatML íƒœê·¸ ë° ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°"""
        # ChatML íƒœê·¸ ì œê±°
        text = text.replace("<|im_start|>assistant\n", "").replace("<|im_end|>", "")
        
        # FIM íƒœê·¸ ì²˜ë¦¬ (ë§Œì•½ ìˆì„ ê²½ìš°)
        if "<|fim_middle|>" in text:
            text = text.replace("<|fim_middle|>", "")
        if "<|fim_prefix|>" in text:
            text = text.replace("<|fim_prefix|>", "")
        if "<|fim_suffix|>" in text:
            text = text.replace("<|fim_suffix|>", "")
        
        # ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì œê±°
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            if line.strip() and not line.startswith('<|'):
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()

def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="DeepSeek-Coder ì½”ë“œ ì–´ì‹œìŠ¤íŠ¸ ì¶”ë¡  (T4 ìµœì í™”)")
    parser.add_argument(
        "--model", 
        type=str, 
        default="../models/prompt-finetuned/final_model", 
        help="ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” ëª¨ë¸ ID"
    )
    parser.add_argument(
        "--mode", 
        type=str, 
        choices=["prompt", "autocomplete", "comment", "error_fix"], 
        default="prompt",
        help="ì½”ë“œ ìƒì„± ëª¨ë“œ"
    )
    parser.add_argument(
        "--temp", 
        type=float, 
        default=0.7,
        help="ì˜¨ë„ ì„¤ì •"
    )
    parser.add_argument(
        "--no-4bit", 
        action="store_true",
        help="4-bit ì–‘ìí™” ë¹„í™œì„±í™”"
    )
    return parser.parse_args()

def main():
    """ë©”ì¸ í•¨ìˆ˜ (Instruction, Input ì§€ì›)"""
    args = parse_arguments()
    
    # ì„¤ì • êµ¬ì„±
    config = MODEL_CONFIG.copy()
    config["temperature"] = args.temp
    config["model_path"] = args.model
    config["load_in_4bit"] = not args.no_4bit
    
    mode_names = {
        "prompt": "[ì œ2ì°¨] ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì½”ë“œ ìƒì„±",
        "autocomplete": "[ì œ1ì°¨] ì½”ë“œ ìë™ì™„ì„±", 
        "comment": "[ì œ3ì°¨] ì£¼ì„ ê¸°ë°˜ ì½”ë“œ ìƒì„±",
        "error_fix": "[ì œ4ì°¨] ì˜¤ë¥˜ ì½”ë“œ ì„¤ëª… ë° ìˆ˜ì •"
    }
    
    print(f"\nğŸ¤– DeepSeek-Coder ì½”ë“œ ì–´ì‹œìŠ¤íŠ¸ ({mode_names.get(args.mode, args.mode)})")
    print("ğŸ“ ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit'ì„ ì…ë ¥í•˜ì„¸ìš”.")
    print("ğŸ’¡ T4 í™˜ê²½ì— ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    print("ğŸ’¡ Instructionê³¼ Inputì„ ë¶„ë¦¬í•´ì„œ ì…ë ¥í•˜ë ¤ë©´ 'instruction:' í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n")
    
    # ì½”ë“œ ì–´ì‹œìŠ¤íŠ¸ ì´ˆê¸°í™”
    assistant = CodeAssistant(config, mode=args.mode)
    
    # REPL ë£¨í”„
    while True:
        try:
            prompts = {
                "comment": "ğŸ’¬ ì£¼ì„ ì…ë ¥ (ì½”ë“œ ìƒì„±): ",
                "autocomplete": "ğŸ’¬ ì½”ë“œ ì…ë ¥ (ìë™ì™„ì„±): ",
                "error_fix": "ğŸ’¬ ì˜¤ë¥˜ ì½”ë“œ ì…ë ¥ (ìˆ˜ì •): ",
                "prompt": "ğŸ’¬ í”„ë¡¬í”„íŠ¸ (instruction: í˜•ì‹ ê°€ëŠ¥): "
            }
            
            user_input = input(prompts.get(args.mode, prompts["prompt"]))
                
            if user_input.lower() in ["exit", "quit"]:
                print("ğŸ‘‹ ì½”ë“œ ì–´ì‹œìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
            if not user_input.strip():
                continue
            
            # instruction: í˜•ì‹ íŒŒì‹±
            parsed_input = user_input
            if "instruction:" in user_input.lower():
                try:
                    parts = user_input.split("instruction:", 1)[1].strip()
                    if "input:" in parts.lower():
                        instruction_part, input_part = parts.split("input:", 1)
                        parsed_input = {
                            "instruction": instruction_part.strip(),
                            "input": input_part.strip()
                        }
                    else:
                        parsed_input = {
                            "instruction": parts.strip(),
                            "input": ""
                        }
                except:
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
                    parsed_input = user_input
                
            print("\nâ³ ìƒì„± ì¤‘...")
            output = assistant.generate(parsed_input, mode=args.mode)
            print(f"\nğŸ“„ ì¶œë ¥:\n{output}\n")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì½”ë“œ ì–´ì‹œìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if "out of memory" in str(e).lower():
                print("ğŸ’¡ ë©”ëª¨ë¦¬ ë¶€ì¡± - ë” ì§§ì€ ì…ë ¥ì„ ì‹œë„í•˜ê±°ë‚˜ --no-4bit ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()