#!/usr/bin/env python3
"""
DeepSeek-Coder 6.7B QLoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ (ì™„ì „ í˜¸í™˜ì„± ë³´ì¥)
- ëª¨ë“  TRL/Transformers ë²„ì „ ì§€ì›
- T4 GPU ìµœì í™”
- save_steps: 10 ì§€ì›
"""

import os
import sys
import json
import yaml
import logging
import argparse
import tempfile
import inspect
from datetime import datetime
from typing import Dict, Any, Optional

import torch
import transformers
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments,
    BitsAndBytesConfig,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset, Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# TRL ì„í¬íŠ¸ (ë²„ì „ë³„ ì²˜ë¦¬)
try:
    from trl import SFTTrainer
    TRL_AVAILABLE = True
except ImportError:
    TRL_AVAILABLE = False
    print("âš ï¸ TRL ì—†ìŒ - ê¸°ë³¸ Trainer ì‚¬ìš©")

# ë¡œê¹… ì„¤ì •
# logs ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('logs', exist_ok=True)

# í˜„ì¬ ì‹œê°„ì„ ë¡œê·¸ íŒŒì¼ëª…ì— ì¶”ê°€
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = f"logs/training_{current_time}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(log_filename, mode='a')
    ]
)
log = logging.getLogger(__name__)

class DeepSeekTrainer:
    """DeepSeek-Coder 6.7B QLoRA íŒŒì¸íŠœë‹ í´ë˜ìŠ¤ (ì™„ì „ í˜¸í™˜ì„±)"""
    
    def __init__(self, config_path: str = "config.yaml", mode: str = "prompt"):
        self.mode = mode
        self.cfg = self.load_config(config_path)
        self.model = None
        self.tok = None
        self.train_ds = None
        self.eval_ds = None
        
        # T4 í™˜ê²½ ìµœì í™” ì„¤ì • ì ìš©
        self.optimize_for_t4()
        
        log.info(f"ğŸš€ DeepSeek-Coder 6.7B íŒŒì¸íŠœë‹ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë“œ: {mode})")
    
    def get_int_config(self, key, default=0):
        """configì—ì„œ ê°’ì„ ê°€ì ¸ì™€ intë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        value = self.cfg.get(key, default)
        if isinstance(value, str):
            try:
                value = int(value)
            except ValueError:
                log.warning(f"âš ï¸ ê²½ê³ : {key} ê°’ '{value}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
                value = default
        return value
    
    def get_float_config(self, key, default=0.0):
        """configì—ì„œ ê°’ì„ ê°€ì ¸ì™€ floatë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        value = self.cfg.get(key, default)
        if isinstance(value, str):
            try:
                value = float(value)
            except ValueError:
                log.warning(f"âš ï¸ ê²½ê³ : {key} ê°’ '{value}'ì„ ì‹¤ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
                value = default
        return value
        
    def optimize_for_t4(self):
        """T4 GPU í™˜ê²½ì— ë§ëŠ” ìµœì í™” ì„¤ì •"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            compute_cap = torch.cuda.get_device_capability(0)
            
            log.info(f"ğŸ–¥ï¸ GPU: {gpu_name}")
            log.info(f"ğŸ”§ Compute Capability: {compute_cap[0]}.{compute_cap[1]}")
            
            # T4ëŠ” Compute Capability 7.5ì´ë¯€ë¡œ bfloat16 ë¯¸ì§€ì›
            if compute_cap[0] < 8:
                log.info("âš ï¸ bfloat16 ë¯¸ì§€ì› GPU - float16 ì‚¬ìš©")
                self.cfg["bnb_4bit_compute_dtype"] = "float16"
                self.cfg["torch_dtype"] = "float16"
                self.cfg["fp16"] = True
                self.cfg["bf16"] = False
            
            # T4 ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤ (15GB)
            if "T4" in gpu_name:
                log.info("ğŸ”§ T4 í™˜ê²½ ìµœì í™” ì ìš©")
                self.cfg["batch_size"] = 1
                self.cfg["grad_acc"] = max(8, self.get_int_config("grad_acc", 4))
                self.cfg["max_length"] = min(1024, self.get_int_config("max_length", 2048))
    
    @staticmethod
    def default_cfg():
        """T4 í™˜ê²½ ìµœì í™”ëœ ê¸°ë³¸ ì„¤ì •"""
        return {
            # ëª¨ë¸ ì„¤ì • - ë¡œì»¬ ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
            "model_name": "/home/ubuntu/deepseek-coder/model_cache/deepseek-coder-6.7b-instruct",
            "torch_dtype": "float16",
            
            # ì–‘ìí™” ì„¤ì • (T4 ìµœì í™”)
            "load_in_4bit": True,
            "bnb_4bit_compute_dtype": "float16",
            "bnb_4bit_quant_type": "nf4",
            "bnb_4bit_use_double_quant": True,
            
            # LoRA ì„¤ì •
            "lora_r": 16,
            "lora_alpha": 32,
            "lora_dropout": 0.1,
            "lora_target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            
            # í•™ìŠµ ì„¤ì • (T4 ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤)
            "batch_size": 1,
            "grad_acc": 8,
            "learning_rate": 2e-4,
            "num_epochs": 3,
            "max_length": 2048,
            "warmup_ratio": 0.1,
            "lr_scheduler": "cosine",
            "weight_decay": 0.01,
            
            # ìµœì í™” ì„¤ì •
            "fp16": True,
            "bf16": False,
            "gradient_checkpointing": True,
            "dataloader_num_workers": 4,
            "remove_unused_columns": False,
            
            # ì €ì¥ ì„¤ì • (save_steps: 10)
            "save_steps": 10,
            "eval_steps": 50,
            "logging_steps": 5,
            "save_total_limit": 5,
            
            # ë°ì´í„° ì„¤ì •
            "data_path": "./data/train.jsonl"
        }
    
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë”©"""
        default_config = self.default_cfg()
        
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                default_config.update(user_config)
                log.info(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë”©: {config_path}")
            except Exception as e:
                log.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        else:
            log.info("ğŸ“ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        
        return default_config
    
    def load_model(self):
        """T4 ìµœì í™”ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©"""
        log.info("ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘")
        log.info("â–¶ï¸  ë² ì´ìŠ¤ ëª¨ë¸ 4-bit ë¡œë“œ")
        
        # ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=self.cfg["load_in_4bit"],
            bnb_4bit_compute_dtype=getattr(torch, self.cfg["bnb_4bit_compute_dtype"]),
            bnb_4bit_quant_type=self.cfg["bnb_4bit_quant_type"],
            bnb_4bit_use_double_quant=self.cfg["bnb_4bit_use_double_quant"],
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        model_path = self.cfg["model_name"]
        
        # ê²½ë¡œ í™•ì¸
        if not model_path.startswith('/'):
            log.warning(f"âš ï¸ ê²½ê³ : ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹™ë‹ˆë‹¤. '/' ì¶”ê°€: {model_path}")
            model_path = '/' + model_path
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if os.path.exists(model_path):
            log.info(f"âœ… ëª¨ë¸ ê²½ë¡œ í™•ì¸ ì„±ê³µ: {model_path}")
            is_local_path = True
        else:
            log.warning(f"âš ï¸ ê²½ê³ : ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            # ê·¸ë˜ë„ ë¡œì»¬ ê²½ë¡œë¡œ ì„¤ì • (Hugging Faceë¡œ ì¸ì‹ ë°©ì§€)
            is_local_path = True
        
        log.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {model_path} (ë¡œì»¬ ê²½ë¡œ: {is_local_path})")
        
        # config.json íŒŒì¼ í™•ì¸
        config_path = os.path.join(model_path, 'config.json')
        tokenizer_path = os.path.join(model_path, 'tokenizer.json')
        if os.path.exists(config_path) and os.path.exists(tokenizer_path):
            log.info(f"âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸: config.json, tokenizer.json ë°œê²¬")
        else:
            files = os.listdir(model_path) if os.path.exists(model_path) else []
            log.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì •ë³´: {files}")
        
        try:
            self.tok = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="right",
                local_files_only=True
            )
            log.info(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
        except Exception as e:
            log.error(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì˜¤ë¥˜: {e}")
            raise  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tok.pad_token is None:
            self.tok.pad_token = self.tok.eos_token
            self.tok.pad_token_id = self.tok.eos_token_id
        
        # ëª¨ë¸ ë¡œë”©
        log.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        
        # ëª¨ë¸ íŒŒì¼ í™•ì¸
        files = os.listdir(model_path)
        log.info(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ íŒŒì¼ ëª©ë¡: {files}")
        
        # ìƒ¤ë“œ íŒŒì¼ í™•ì¸
        shard_files = [f for f in files if 'model.safetensors.index.json' in f or 
                      ('model' in f and '.safetensors' in f) or 
                      ('pytorch_model' in f and '.bin' in f)]
        log.info(f"ëª¨ë¸ ìƒ¤ë“œ íŒŒì¼: {shard_files}")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        if torch.cuda.is_available():
            log.info(f"CUDA ë©”ëª¨ë¦¬ ìƒíƒœ (ëª¨ë¸ ë¡œë“œ ì „): ì‚¬ìš© ì¤‘: {torch.cuda.memory_allocated()/1024**3:.2f}GB, ì˜ˆì•½ë¨: {torch.cuda.memory_reserved()/1024**3:.2f}GB")
        
        import time
        start_time = time.time()
        log.info("ëª¨ë¸ ë¡œë”© ì‹œì‘ - ì´ ê³¼ì •ì€ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        
        try:
            # ë¡œë”© ì„¤ì • ì—…ë°ì´íŠ¸ - ë” ëŠë¦¬ì§€ë§Œ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=getattr(torch, self.cfg["torch_dtype"]),
                use_cache=False,
                local_files_only=True,
                low_cpu_mem_usage=True,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
                offload_folder="offload_folder"  # í•„ìš”ì‹œ ë””ìŠ¤í¬ë¡œ ì˜¤í”„ë¡œë“œ
            )
            end_time = time.time()
            log.info(f"ëª¨ë¸ ë¡œë”© ì„±ê³µ! ì†Œìš” ì‹œê°„: {(end_time-start_time):.2f}ì´ˆ")
            
            if torch.cuda.is_available():
                log.info(f"CUDA ë©”ëª¨ë¦¬ ìƒíƒœ (ëª¨ë¸ ë¡œë“œ í›„): ì‚¬ìš© ì¤‘: {torch.cuda.memory_allocated()/1024**3:.2f}GB, ì˜ˆì•½ë¨: {torch.cuda.memory_reserved()/1024**3:.2f}GB")
                
        except Exception as e:
            log.error(f"ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}")
            import traceback
            log.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            raise  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
        
        # k-bit í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„
        self.model = prepare_model_for_kbit_training(
            self.model, 
            use_gradient_checkpointing=self.cfg["gradient_checkpointing"]
        )
        log.info("âœ… ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ìœ¼ë¡œ k-bit í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ.")
        
        # LoRA ì„¤ì • ë° ì ìš©
        lora_config = LoraConfig(
            r=self.cfg["lora_r"],
            lora_alpha=self.cfg["lora_alpha"],
            target_modules=self.cfg["lora_target_modules"],
            lora_dropout=self.cfg["lora_dropout"],
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        log.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def load_data(self):
        """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"""
        log.info("ğŸ”„ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì‹œì‘")
        log.info(f"â–¶ï¸  ë°ì´í„° ë¡œë”©: {self.cfg['data_path']}")
        
        # JSONL íŒŒì¼ ë¡œë”©
        dataset = load_dataset("json", data_files=self.cfg["data_path"], split="train")
        
        log.info(f"ğŸ“Š ë°ì´í„°ì…‹ ì»¬ëŸ¼: {dataset.column_names}")
        log.info(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ ìƒ˜í”Œ")
        
        # ì²« ìƒ˜í”Œ ë‚´ìš© ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if len(dataset) > 0:
            first_sample = dataset[0]
            if "messages" in first_sample and len(first_sample["messages"]) > 0:
                log.debug(f"ì²« ë²ˆì§¸ ìƒ˜í”Œ ë©”ì‹œì§€ êµ¬ì¡°: {first_sample['messages'][0].keys()}")
        
        # ë°ì´í„° í˜•ì‹ì„ ê°ì§€í•˜ê³  ì ì ˆí•œ ë³€í™˜ í•¨ìˆ˜ ì„ íƒ
        def detect_format(example):
            """ë°ì´í„° í˜•ì‹ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜"""
            if "messages" in example:
                # messages ë°°ì—´ì´ ìˆê³ , ë°°ì—´ í˜•ì‹ì´ë©°, ê° ë©”ì‹œì§€ëŠ” roleê³¼ contentë¥¼ ê°€ì§
                if (len(example["messages"]) > 0 and 
                    isinstance(example["messages"], list) and 
                    isinstance(example["messages"][0], dict) and 
                    "role" in example["messages"][0] and 
                    "content" in example["messages"][0]):
                    return "chat"
            
            elif "prompt" in example and "completion" in example:
                return "prompt_completion"
                
            elif "prefix_code" in example and "suffix_code" in example and "comment" in example and "target_code" in example:
                # ì£¼ì„ ê¸°ë°˜ ì½”ë“œ ìƒì„± í˜•ì‹
                return "comment_to_code"
                
            elif "error_context" in example and "explanation" in example:
                return "error_explanation"
                
            elif "error_context" in example and "fixed_code_snippet" in example:
                # error_fix í˜•ì‹ ê°ì§€
                error_context = example["error_context"]
                if isinstance(error_context, dict) and "buggy_code_snippet" in error_context:
                    return "error_fix"
                # ë˜ëŠ” buggy_code_snippetì´ ìµœìƒìœ„ í•„ë“œì— ìˆëŠ” ê²½ìš°
                elif "buggy_code_snippet" in example:
                    return "error_fix"
                    
            elif "instruction" in example and "input" in example and "output" in example:
                return "instruction_input_output"
                
            else:
                # ë°ì´í„° êµ¬ì¡° ë¡œê¹…
                log.warning(f"\u26a0ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° í˜•ì‹: {list(example.keys())}")
                return "unknown"
        
        # ìƒ˜í”Œì„ ê²€ì‚¬í•˜ì—¬ ë°ì´í„° í˜•ì‹ ê°ì§€
        sample = dataset[0]
        data_format = detect_format(sample)
        log.info(f"âœ… ê°ì§€ëœ ë°ì´í„° í˜•ì‹: {data_format}")
        
        # FIM í˜•ì‹ ì‚¬ìš© ì—¬ë¶€ë¥¼ ë¯¸ë¦¬ í™•ì¸ (ë¡œê·¸ ì¤‘ë³µ ë°©ì§€)
        has_fim_format = False
        if data_format == "prompt_completion" and self.mode == "comment":
            # ì²« ëª‡ ê°œ ìƒ˜í”Œ ê²€ì‚¬í•˜ì—¬ FIM íƒœê·¸ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            sample_check_count = min(5, len(dataset))
            for i in range(sample_check_count):
                sample_prompt = dataset[i]["prompt"]
                if "<|fim begin|>" in sample_prompt or "<|fim hole|>" in sample_prompt or "<|fim end|>" in sample_prompt:
                    log.info("âœ… FIM í˜•ì‹ ì£¼ì„ ê¸°ë°˜ ëª¨ë¸ ë°ì´í„° ê°ì§€ë¨")
                    has_fim_format = True
                    break
        
        # í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
        def tokenize_function(examples):
            # ë°ì´í„° í˜•ì‹ì— ë”°ë¼ ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            texts = []
            
            # ìƒ˜í”Œ ìˆ˜ ê²°ì • (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
            if "messages" in examples:
                sample_count = len(examples["messages"])
            elif "prompt" in examples:
                sample_count = len(examples["prompt"])
            elif "prefix_code" in examples:
                sample_count = len(examples["prefix_code"])
            elif "error_context" in examples:
                sample_count = len(examples["error_context"])
            elif "instruction" in examples:
                sample_count = len(examples["instruction"])
            else:
                log.error("âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹")
                raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤.")
                
            # ë°ì´í„° í˜•ì‹ ë¡œê¹…
            log.info(f"ğŸ”¢ ìƒ˜í”Œ ìˆ˜: {sample_count}, í˜•ì‹: {data_format}")
            
            for i in range(sample_count):
                # ë°ì´í„° í˜•ì‹ë³„ ì²˜ë¦¬
                if data_format == "chat":
                    # ì±„íŒ… í˜•ì‹ (messages êµ¬ì¡°) - Qwen2.5 ëª¨ë¸ í˜•ì‹ ì§€ì›
                    messages = examples["messages"][i]
                    chat_text = ""
                    
                    # ëª¨ë“  ë©”ì‹œì§€ë¥¼ ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    for msg in messages:
                        role = msg["role"]
                        content = msg["content"]
                        chat_text += f"<|im_start|>{role}\n{content}<|im_end|>\n"
                    
                    texts.append(chat_text.strip())
                
                elif data_format == "prompt_completion":
                    # ì£¼ì„ í‚¤ì›Œë“œ ê¸°ë°˜ ì½”ë“œ ìƒì„± í˜•ì‹
                    prompt = examples["prompt"][i]
                    completion = examples["completion"][i]
                    
                    # FIM í˜•ì‹ ì²˜ë¦¬ (3ì°¨ ì£¼ì„ ê¸°ë°˜ ëª¨ë¸)
                    if self.mode == "comment" and ("<|fim begin|>" in prompt or "<|fim hole|>" in prompt or "<|fim end|>" in prompt):
                        # FIM í˜•ì‹ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ChatML í˜•ì‹ìœ¼ë¡œ ë˜í•‘
                        text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n{completion}<|im_end|>"
                        texts.append(text)
                    else:
                        # ì¼ë°˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ, ì™„ì„±ì„ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¡œ ë³€í™˜
                        text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n{completion}<|im_end|>"
                    
                    texts.append(text)
                
                elif data_format == "error_explanation":
                    # ì—ëŸ¬ ì„¤ëª… í˜•ì‹
                    error_context = examples["error_context"][i]
                    explanation = examples["explanation"][i]
                    
                    # ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±
                    error_log = error_context.get("error_log", "")
                    code_snippet = error_context.get("code_snippet", "")
                    language = error_context.get("language", "")
                    
                    # ì‚¬ìš©ì ì…ë ¥ êµ¬ì„±
                    user_text = f"ë‹¤ìŒ {language} ì½”ë“œì˜ ì—ëŸ¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”:\n\nì—ëŸ¬ ë¡œê·¸:\n{error_log}\n\nì½”ë“œ:\n{code_snippet}"
                    
                    # ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    text = f"<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n{explanation}<|im_end|>"
                    texts.append(text)
                
                elif data_format == "comment_to_code":
                    # ì£¼ì„ ê¸°ë°˜ ì½”ë“œ ìƒì„± í˜•ì‹ (ìƒˆë¡œìš´ ë°ì´í„° êµ¬ì¡°)
                    prefix_code = examples["prefix_code"][i]
                    suffix_code = examples["suffix_code"][i]
                    comment = examples["comment"][i]
                    # instructionì´ ì—†ëŠ” ê²½ìš° ì£¼ì„ì„ í™œìš©
                    instruction = examples.get("instruction", [comment] * sample_count)[i]
                    target_code = examples["target_code"][i]
                    
                    # ì‚¬ìš©ì ì…ë ¥ êµ¬ì„± (ì£¼ì„ê³¼ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸)
                    user_text = f"ì£¼ì„ì— ë”°ë¼ ì ì ˆí•œ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n\n"
                    user_text += f"ì£¼ì„: {comment}\n\n"
                    if instruction and instruction != comment:
                        user_text += f"ì§€ì‹œì‚¬í•­: {instruction}\n\n"
                    user_text += "\nì´ì „ ì½”ë“œ:\n"
                    user_text += f"{prefix_code}\n"
                    user_text += "// ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‚½ì…í•´ì•¼ í•¨ //\n"
                    user_text += f"{suffix_code}"
                    
                    # ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    text = f"<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n{target_code}<|im_end|>"
                    texts.append(text)
                
                elif data_format == "error_fix":
                    # ì—ëŸ¬ ìˆ˜ì • í˜•ì‹
                    error_context = examples["error_context"][i]
                    fixed_code = examples["fixed_code_snippet"][i]
                    
                    # ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±
                    error_log = error_context.get("error_log", "")
                    language = error_context.get("language", "")
                    title = error_context.get("title", "")
                    description = error_context.get("description", "")
                    
                    # buggy_codeê°€ error_context ì•ˆì— ìˆëŠ” ê²½ìš°ì™€ ì™¸ë¶€ì— ìˆëŠ” ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
                    if "buggy_code_snippet" in error_context:
                        buggy_code = error_context["buggy_code_snippet"]
                    elif "buggy_code_snippet" in examples:
                        buggy_code = examples["buggy_code_snippet"][i]
                    else:
                        buggy_code = ""
                        log.warning("\u26a0ï¸ buggy_code_snippetì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
                    # ì‚¬ìš©ì ì…ë ¥ êµ¬ì„± - ì œëª©ê³¼ ì„¤ëª… ì¶”ê°€
                    user_text = f"ë‹¤ìŒ {language} ì½”ë“œì˜ ì—ëŸ¬ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n"
                    if title:
                        user_text += f"\nì˜¤ë¥˜: {title}"
                    if description:
                        user_text += f"\nì„¤ëª…: {description}"
                    user_text += f"\n\nì—ëŸ¬ ë¡œê·¸:\n{error_log}\n\nì½”ë“œ:\n{buggy_code}"
                    
                    # ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    text = f"<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n{fixed_code}<|im_end|>"
                    texts.append(text)
                    
                elif data_format == "instruction_input_output":
                    # ê¸°ì¡´ instruction, input, output í˜•ì‹
                    instruction = examples["instruction"][i].strip()
                    input_text = examples["input"][i].strip()
                    output_text = examples["output"][i].strip()
                    
                    # íƒœê·¸ ì •ë³´ ì¶”ê°€ (ìˆë‹¤ë©´)
                    tags = examples.get("tags", [None] * len(examples["instruction"]))[i]
                    if tags:
                        if isinstance(tags, str):
                            tags = [tags]
                        tag_info = f"Task Type: {', '.join(tags)}\n"
                        if input_text:
                            user_text = f"{tag_info}Instruction: {instruction}\nInput: {input_text}"
                        else:
                            user_text = f"{tag_info}Instruction: {instruction}"
                    else:
                        if input_text:
                            user_text = f"Instruction: {instruction}\nInput: {input_text}"
                        else:
                            user_text = f"Instruction: {instruction}"
                    
                    # ChatML í˜•ì‹
                    text = f"<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"
                    texts.append(text)
                
                else:
                    log.error(f"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: {data_format}")
                    raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: {data_format}")
            
            # FIM í˜•ì‹ íŠ¹ìˆ˜ ì²˜ë¦¬ (promptì— <|fim begin|>, <|fim hole|>, <|fim end|> íƒœê·¸ê°€ ìˆëŠ” ê²½ìš°)
            if data_format == "prompt_completion":
                for i in range(len(texts)):
                    prompt = examples["prompt"][i]
                    if "<ï½œfim beginï½œ>" in prompt and "<ï½œfim holeï½œ>" in prompt and "<ï½œfim endï½œ>" in prompt:
                        # FIM í˜•ì‹ì˜ íŠ¹ìˆ˜ ì²˜ë¦¬
                        completion = examples["completion"][i]
                        
                        # fim beginê³¼ fim hole ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (prefix)
                        prefix = prompt.split("<ï½œfim holeï½œ>")[0].replace("<ï½œfim beginï½œ>", "")
                        # fim holeê³¼ fim end ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (suffix)
                        suffix = prompt.split("<ï½œfim holeï½œ>")[1].split("<ï½œfim endï½œ>")[0]
                        
                        # FIM í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ì§ì ‘ ë³€í™˜
                        text = f"<|fim_prefix|>{prefix}<|fim_suffix|>{suffix}<|fim_middle|>{completion}"
                        texts[i] = text
            
            # í† í¬ë‚˜ì´ì§•
            model_inputs = self.tok(
                texts,
                truncation=True,
                padding=True,  # Trueë¡œ ë³€ê²½í•˜ì—¬ ëª¨ë“  ì‹œí€€ìŠ¤ê°€ ë™ì¼ ê¸¸ì´ë¥¼ ê°–ë„ë¡ í•¨
                max_length=self.cfg["max_length"],
                return_tensors=None
            )
            
            # labels = input_ids (causal LM)
            model_inputs["labels"] = model_inputs["input_ids"].copy()
            
            return model_inputs
        
        # ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•
        dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
            desc="í† í¬ë‚˜ì´ì§• ì¤‘"
        )
        
        log.info("âœ… ë°ì´í„° í† í¬ë‚˜ì´ì§• ì™„ë£Œ")
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
        self.train_ds = split_dataset["train"]
        self.eval_ds = split_dataset["test"]
        
        log.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(self.train_ds)}ê°œ")
        log.info(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(self.eval_ds)}ê°œ")
    
    def t_args(self) -> TrainingArguments:
        """ì™„ì „ í˜¸í™˜ í•™ìŠµ ì¸ì ìƒì„±"""
        # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½ (AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹œ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì„ ìœ„í•¨)
        # í•™ìŠµ ê²½ë¡œ ì„¤ì • - ì²´í¬í¬ì¸íŠ¸ëŠ” ì›ë˜ ê²½ë¡œì— ì €ì¥
        scripts_dir = "/home/ubuntu/deepseek-coder/scripts"
        
        # í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ì™€ ì¶œë ¥ ê²½ë¡œ
        output_dirs = {
            "complete": f"{scripts_dir}/output/autocomplete-finetuned",
            "prompt": f"{scripts_dir}/output/prompt-finetuned",
            "comment": f"{scripts_dir}/output/comment-finetuned",
            "error_fix": f"{scripts_dir}/output/error-fix-finetuned"
        }
        
        # ìµœì¢… ëª¨ë¸ ê²½ë¡œ (ì¶”ë¡ ìš©)
        models_dir = "../models"
        final_model_dirs = {
            "complete": f"{models_dir}/autocomplete-finetuned",
            "prompt": f"{models_dir}/prompt-finetuned",
            "comment": f"{models_dir}/comment-finetuned",
            "error_fix": f"{models_dir}/error-fix-finetuned"
        }
        
        # í•™ìŠµ ê²½ë¡œ (ì²´í¬í¬ì¸íŠ¸ì™€ ì¶œë ¥ ë””ë ‰í† ë¦¬)
        output_dir = output_dirs.get(self.mode, f"{scripts_dir}/output/prompt-finetuned")
        
        # ìµœì¢… ëª¨ë¸ ê²½ë¡œ (inference.pyì—ì„œ ì‚¬ìš©í•  ê²½ë¡œ) ë³„ë„ ì €ì¥
        self.final_model_dir = final_model_dirs.get(self.mode, f"{models_dir}/prompt-finetuned")
        
        # ìµœì†Œí•œì˜ ì•ˆì „í•œ ì¸ìë§Œ ì‚¬ìš© (íƒ€ì… ë³€í™˜ ì¶”ê°€)
        return TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=self.get_int_config("batch_size", 4),  # config.yamlì—ì„œëŠ” 4
            per_device_eval_batch_size=self.get_int_config("batch_size", 4),  # config.yamlì—ì„œëŠ” 4
            gradient_accumulation_steps=self.get_int_config("grad_acc", 2),  # config.yamlì—ì„œëŠ” 2
            learning_rate=self.get_float_config("learning_rate", 2e-4),
            num_train_epochs=self.get_float_config("num_epochs", 3),
            warmup_ratio=self.get_float_config("warmup_ratio", 0.1),
            lr_scheduler_type=self.cfg["lr_scheduler"],
            weight_decay=self.get_float_config("weight_decay", 0.01),
            fp16=self.cfg["fp16"],
            bf16=self.cfg["bf16"],
            gradient_checkpointing=self.cfg["gradient_checkpointing"],
            dataloader_num_workers=self.get_int_config("dataloader_num_workers", 4),
            save_steps=self.get_int_config("save_steps", 10),  # config.yamlì—ì„œëŠ” 10
            eval_steps=self.get_int_config("eval_steps", 50),  # config.yamlì—ì„œëŠ” 50
            logging_steps=self.get_int_config("logging_steps", 5),  # config.yamlì—ì„œëŠ” 5
            save_total_limit=self.get_int_config("save_total_limit", 3),
            # ì§€ì›ë˜ì§€ ì•ŠëŠ” ì¸ì ì œê±°: evaluation_strategy, save_strategy, load_best_model_at_end, metric_for_best_model
            remove_unused_columns=self.cfg["remove_unused_columns"],
            report_to="none",  # Noneì´ ì•„ë‹Œ "none"ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ë¡œê¹… ë¹„í™œì„±í™”
            run_name=f"deepseek-coder-{self.mode}-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
    
    def train(self):
        """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
        log.info(f"ğŸš€ í•™ìŠµ ì‹œì‘: {self.mode} ëª¨ë“œ")
        
        # í•™ìŠµ ì¸ì ì¤€ë¹„
        args = self.t_args()
        
        # ì²´í¬í¬ì¸íŠ¸ ê°ì§€ ë° ìë™ ì¬ê°œ ê¸°ëŠ¥
        checkpoint_dir = args.output_dir
        resume_from_checkpoint = None
        
        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        if os.path.exists(checkpoint_dir):
            checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith("checkpoint-")]
            
            if checkpoints:
                # ì²´í¬í¬ì¸íŠ¸ ë²ˆí˜¸ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                sorted_checkpoints = sorted(
                    checkpoints,
                    key=lambda x: int(x.split("-")[1]) if len(x.split("-")) > 1 and x.split("-")[1].isdigit() else 0
                )
                
                if sorted_checkpoints:
                    latest_checkpoint = sorted_checkpoints[-1]
                    resume_from_checkpoint = os.path.join(checkpoint_dir, latest_checkpoint)
                    log.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ê°ì§€: {resume_from_checkpoint}ì—ì„œ í•™ìŠµ ì¬ê°œ")
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tok,
            mlm=False,  # Causal LM
            pad_to_multiple_of=8
        )
        
        # Trainer ì„¤ì • - resume_from_checkpointëŠ” train() ë©”ì„œë“œì—ë§Œ ì „ë‹¬
        trainer = Trainer(
            model=self.model,
            args=args,
            train_dataset=self.train_ds,
            eval_dataset=self.eval_ds,
            data_collator=data_collator,
            tokenizer=self.tok
        )
        
        log.info("âœ… Trainer ì´ˆê¸°í™” ì™„ë£Œ, ì²´í¬í¬ì¸íŠ¸ ê°ì§€ í™œì„±í™”")
        
        # í•™ìŠµ ì‹¤í–‰
        try:
            trainer.train(resume_from_checkpoint=resume_from_checkpoint)
            log.info("âœ… í•™ìŠµ ì™„ë£Œ")
            
            # 1. ìµœì¢… ëª¨ë¸ ì €ì¥ - args.output_dir ë‚´ì˜ final_model ê²½ë¡œì— ì €ì¥
            output_model_path = os.path.join(args.output_dir, "final_model")
            trainer.save_model(output_model_path)
            self.tok.save_pretrained(output_model_path)
            
            log.info(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_model_path}")
            
            # 2. inference.py í˜¸í™˜ì„ ìœ„í•´ ëª¨ë¸ì„ ../models ê²½ë¡œë¡œë„ ë³µì‚¬
            import shutil
            final_model_path = os.path.join(self.final_model_dir, "final_model")
            
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            os.makedirs(os.path.dirname(final_model_path), exist_ok=True)
            
            if os.path.exists(output_model_path) and output_model_path != final_model_path:
                try:
                    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ í›„ ë³µì‚¬
                    if os.path.exists(final_model_path):
                        shutil.rmtree(final_model_path)
                    
                    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ë³µì‚¬
                    shutil.copytree(output_model_path, final_model_path)
                    log.info(f"âœ… inference.py í˜¸í™˜ì„ ìœ„í•´ ëª¨ë¸ ë³µì‚¬ ì™„ë£Œ: {final_model_path}")
                except Exception as e:
                    log.error(f"âŒ ëª¨ë¸ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            return final_model_path
            
        except Exception as e:
            log.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def run(self):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            self.load_model()
            self.load_data()
            final_model_path = self.train()
            
            log.info("ğŸ‰ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            return final_model_path
            
        except Exception as e:
            log.error(f"âŒ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            raise

def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="DeepSeek-Coder 6.7B QLoRA íŒŒì¸íŠœë‹ (ì™„ì „ í˜¸í™˜ì„±)")
    parser.add_argument("--config", type=str, default="config.yaml", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--mode", type=str, choices=["complete", "prompt", "comment", "error_fix"], default="prompt", help="í•™ìŠµ ëª¨ë“œ")
    return parser.parse_args()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_arguments()
    
    mode_descriptions = {
        "complete": "[ì œ1ì°¨] ì½”ë“œ ìë™ì™„ì„± ì „ìš© (FIM í˜•ì‹)",
        "prompt": "[ì œ2ì°¨] ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì½”ë“œ ìƒì„±",
        "comment": "[ì œ3ì°¨] ì£¼ì„ ê¸°ë°˜ ì½”ë“œ ìƒì„±",
        "error_fix": "[ì œ4ì°¨] ì˜¤ë¥˜ ì½”ë“œ ì„¤ëª… ë° ìˆ˜ì •"
    }
    
    log.info("=" * 53)
    log.info("DeepSeek Coder 6.7B ì™„ì „ í˜¸í™˜ì„± íŒŒì¸íŠœë‹ ì‹œì‘")
    log.info("=" * 53)
    log.info(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%a %b %d %H:%M:%S %Z %Y')}")
    log.info(f"í•™ìŠµ ëª¨ë“œ: [{args.mode}] {mode_descriptions.get(args.mode, args.mode)}")
    
    # í™˜ê²½ ì •ë³´ ì¶œë ¥
    log.info(f"Python ëª…ë ¹ì–´: {sys.executable}")
    if torch.cuda.is_available():
        gpu_info = f"{torch.cuda.get_device_name(0)}, {torch.cuda.get_device_properties(0).total_memory // 1024**2}, {torch.cuda.memory_allocated() // 1024**2}"
        log.info(f"GPU ì •ë³´:\n{gpu_info}")
    
    log.info(f"Python ë²„ì „: Python {sys.version.split()[0]}")
    
    # íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸
    log.info("í™•ì¸ ì¤‘: í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤...")
    packages = ["torch", "transformers", "datasets", "accelerate", "peft"]
    for pkg in packages:
        try:
            module = __import__(pkg)
            version = getattr(module, "__version__", "unknown")
            log.info(f"âœ… {pkg}: {version}")
        except ImportError:
            log.error(f"âŒ {pkg}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    try:
        # í•™ìŠµ ì‹¤í–‰
        trainer = DeepSeekTrainer(config_path=args.config, mode=args.mode)
        final_model_path = trainer.run()
        
        log.info("=" * 53)
        log.info("í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        log.info("=" * 53)
        log.info(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {final_model_path}")
        log.info(f"inference.pyì—ì„œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´: python inference.py --mode {args.mode} --model {final_model_path}")
        
    except KeyboardInterrupt:
        log.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        log.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    finally:
        # ìµœì¢… GPU ìƒíƒœ ì¶œë ¥
        if torch.cuda.is_available():
            log.info("ìµœì¢… GPU ìƒíƒœ:")
            os.system("nvidia-smi")

if __name__ == "__main__":
    main()