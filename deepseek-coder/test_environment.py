#!/usr/bin/env python3
"""
AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ê°ì§€ ë° í™˜ê²½ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (T4 ìµœì í™”)
- ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹ í˜¸ ëª¨ë‹ˆí„°ë§
- GPU ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- íŒ¨í‚¤ì§€ í˜¸í™˜ì„± í™•ì¸
- Instruction, Input, Output ë°ì´í„° êµ¬ì¡° ì§€ì›
- í•™ìŠµ í™˜ê²½ ê²€ì¦
"""

import os
import sys
import time
import json
import signal
import requests
import subprocess
from datetime import datetime
from typing import Dict, Any, List

def signal_handler(signum, frame):
    """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
    print(f"\nâš ï¸ ì‹œê·¸ë„ {signum} ìˆ˜ì‹ ë¨. ì •ë¦¬ ì‘ì—… ì¤‘...")
    cleanup_and_exit()

def cleanup_and_exit():
    """ì •ë¦¬ ì‘ì—… í›„ ì¢…ë£Œ"""
    print("ğŸ§¹ ì •ë¦¬ ì‘ì—… ì™„ë£Œ")
    sys.exit(0)

def check_spot_instance_termination():
    """AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹ í˜¸ í™•ì¸"""
    try:
        # ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ë©”íƒ€ë°ì´í„° í™•ì¸
        response = requests.get(
            "http://169.254.169.254/latest/meta-data/spot/instance-action",
            timeout=2
        )
        
        if response.status_code == 200:
            termination_time = response.text
            print(f"âš ï¸ ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì˜ˆì •: {termination_time}")
            return True
        else:
            return False
            
    except requests.exceptions.RequestException:
        # ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ê°€ ì•„ë‹ˆê±°ë‚˜ ì¤‘ë‹¨ ì‹ í˜¸ ì—†ìŒ
        return False

def test_gpu_environment():
    """GPU í™˜ê²½ í…ŒìŠ¤íŠ¸ (T4 ìµœì í™”)"""
    print("\nGPU í™˜ê²½ í…ŒìŠ¤íŠ¸...")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU ê°œìˆ˜: {gpu_count}")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                compute_cap = torch.cuda.get_device_capability(i)
                
                print(f"   GPU {i}: {gpu_name}")
                print(f"   ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
                print(f"   Compute Capability: {compute_cap[0]}.{compute_cap[1]}")
                
                # T4 íŠ¹ë³„ í™•ì¸
                if "T4" in gpu_name:
                    print("   ğŸ”§ T4 GPU ê°ì§€ - ìµœì í™” ì„¤ì • ì ìš©ë¨")
                    if compute_cap[0] < 8:
                        print("   âš ï¸ bfloat16 ë¯¸ì§€ì› - float16 ì‚¬ìš© ê¶Œì¥")
                
                # bfloat16 ì§€ì› í™•ì¸
                if torch.cuda.is_bf16_supported():
                    print("   âœ… bfloat16 ì§€ì›ë¨")
                else:
                    print("   âš ï¸ bfloat16 ë¯¸ì§€ì› - float16 ì‚¬ìš©")
            
            # ê°„ë‹¨í•œ GPU ì—°ì‚° í…ŒìŠ¤íŠ¸
            try:
                test_tensor = torch.randn(1000, 1000).cuda()
                result = torch.matmul(test_tensor, test_tensor.t())
                print("   âœ… GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ í†µê³¼")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                memory_reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_allocated:.2f}GB (í• ë‹¹) / {memory_reserved:.2f}GB (ì˜ˆì•½)")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del test_tensor, result
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"   âŒ GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                return False
                
        else:
            print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€")
            return False
            
        return True
        
    except ImportError:
        print("âŒ PyTorch ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return False

def test_packages():
    """í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸"""
    print("\níŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸...")
    
    required_packages = {
        "torch": "PyTorch",
        "transformers": "Transformers",
        "datasets": "Datasets", 
        "accelerate": "Accelerate",
        "peft": "PEFT",
        "trl": "TRL",
        "bitsandbytes": "BitsAndBytes"
    }
    
    missing_packages = []
    
    for package, description in required_packages.items():
        try:
            module = __import__(package)
            version = getattr(module, "__version__", "unknown")
            print(f"âœ… {description}: {version}")
            
            # íŠ¹ë³„ í™•ì¸ì‚¬í•­
            if package == "torch":
                import torch
                print(f"   CUDA ë²„ì „: {torch.version.cuda}")
                print(f"   cuDNN ë²„ì „: {torch.backends.cudnn.version()}")
                
        except ImportError:
            print(f"âŒ {description}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nâš ï¸ ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install torch transformers datasets accelerate peft trl bitsandbytes")
        return False
    
    return True

def test_data_file():
    """ë°ì´í„° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (Instruction, Input, Output êµ¬ì¡° ì§€ì›)"""
    print("\në°ì´í„° íŒŒì¼ í™•ì¸...")
    
    # ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ê²½ë¡œë“¤
    possible_paths = [
        "/home/ubuntu/deepseek-coder/data/train.jsonl",  # train.py ê¸°ë³¸ ê²½ë¡œ
        "../data/train.jsonl",
        "./data/train.jsonl"
    ]
    
    data_path = None
    for path in possible_paths:
        if os.path.exists(path):
            data_path = path
            break
    
    if data_path:
        print(f"âœ… ë°ì´í„° íŒŒì¼ ì¡´ì¬: {data_path}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(data_path)
        print(f"   íŒŒì¼ í¬ê¸°: {file_size / (1024*1024):.2f} MB")
        
        # ì´ ë¼ì¸ ìˆ˜ í™•ì¸
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                total_lines = sum(1 for _ in f)
            print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {total_lines:,}ê°œ")
        except Exception as e:
            print(f"   ë¼ì¸ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # ì²« ëª‡ ì¤„ ì½ê¸° í…ŒìŠ¤íŠ¸ ë° êµ¬ì¡° í™•ì¸
        try:
            import json
            with open(data_path, 'r', encoding='utf-8') as f:
                lines = []
                for i, line in enumerate(f):
                    if i >= 3:  # ì²« 3ì¤„ë§Œ
                        break
                    try:
                        lines.append(json.loads(line.strip()))
                    except json.JSONDecodeError as e:
                        print(f"   âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {i+1}): {e}")
                        continue
            
            if lines:
                sample_keys = list(lines[0].keys())
                print(f"   ë°ì´í„° ì»¬ëŸ¼: {sample_keys}")
                
                # ë°ì´í„° êµ¬ì¡° ê²€ì¦
                if all(key in sample_keys for key in ["instruction", "input", "output"]):
                    print("   âœ… Instruction, Input, Output êµ¬ì¡° í™•ì¸ë¨")
                    
                    # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                    sample = lines[0]
                    instruction_preview = str(sample['instruction'])[:50].replace('\n', '\\n')
                    input_preview = str(sample['input'])[:50].replace('\n', '\\n')
                    output_preview = str(sample['output'])[:50].replace('\n', '\\n')
                    
                    print(f"   - Instruction: {instruction_preview}...")
                    print(f"   - Input: {input_preview}...")
                    print(f"   - Output: {output_preview}...")
                    
                    # íƒœê·¸ í™•ì¸ (ìˆë‹¤ë©´)
                    if 'tags' in sample_keys:
                        tags = sample.get('tags', [])
                        print(f"   - Tags: {tags}")
                    
                elif "input" in sample_keys and "output" in sample_keys:
                    print("   âœ… Input, Output êµ¬ì¡° í™•ì¸ë¨ (ë ˆê±°ì‹œ ì§€ì›)")
                    
                    # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                    sample = lines[0]
                    input_preview = str(sample['input'])[:50].replace('\n', '\\n')
                    output_preview = str(sample['output'])[:50].replace('\n', '\\n')
                    
                    print(f"   - Input: {input_preview}...")
                    print(f"   - Output: {output_preview}...")
                    
                    # íƒœê·¸ í™•ì¸ (ìˆë‹¤ë©´)
                    if 'tags' in sample_keys:
                        tags = sample.get('tags', [])
                        print(f"   - Tags: {tags}")
                    
                else:
                    print("   âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° êµ¬ì¡°")
                    print(f"   í•„ìš”í•œ í‚¤: ['instruction', 'input', 'output'] ë˜ëŠ” ['input', 'output']")
                    print(f"   ì‹¤ì œ í‚¤: {sample_keys}")
                    return False
                
                # íƒœê·¸ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
                if 'tags' in sample_keys and len(lines) > 1:
                    tag_counts = {}
                    for line in lines:
                        tags = line.get('tags', [])
                        if isinstance(tags, str):
                            tags = [tags]
                        for tag in tags:
                            tag_counts[tag] = tag_counts.get(tag, 0) + 1
                    
                    if tag_counts:
                        print(f"   íƒœê·¸ë³„ ìƒ˜í”Œ ìˆ˜ (ì²« 3ê°œ ê¸°ì¤€): {tag_counts}")
            
        except Exception as e:
            print(f"âš ï¸  ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return False
            
        return True
    else:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("   í™•ì¸í•œ ê²½ë¡œë“¤:")
        for path in possible_paths:
            print(f"   - {path}")
        return False

def test_model_loading():
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (T4 ìµœì í™”)"""
    print("\nëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸...")
    
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        
        model_name = "deepseek-ai/deepseek-coder-6.7b-base"
        print(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸: {model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë”© í…ŒìŠ¤íŠ¸
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
        
        # T4 í™˜ê²½ì—ì„œ 4-bit ì–‘ìí™” ì„¤ì • í…ŒìŠ¤íŠ¸
        if torch.cuda.is_available():
            try:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,  # T4 í˜¸í™˜
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
                print("âœ… 4-bit ì–‘ìí™” ì„¤ì • ìƒì„± ì„±ê³µ")
                
                # ì‹¤ì œ ëª¨ë¸ ë¡œë”©ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ìŠ¤í‚µ
                print("   (ì‹¤ì œ ëª¨ë¸ ë¡œë”©ì€ í•™ìŠµ ì‹œì— ìˆ˜í–‰ë©ë‹ˆë‹¤)")
                
            except Exception as e:
                print(f"âŒ ì–‘ìí™” ì„¤ì • ì‹¤íŒ¨: {e}")
                return False
        else:
            print("âš ï¸ CUDA ì—†ìŒ - CPU ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸")
        
        return True
        
    except ImportError as e:
        print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì—†ìŒ: {e}")
        return False

def test_disk_space():
    """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸"""
    print("\në””ìŠ¤í¬ ê³µê°„ í™•ì¸...")
    
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
        statvfs = os.statvfs('.')
        free_space = statvfs.f_frsize * statvfs.f_bavail / (1024**3)  # GB
        total_space = statvfs.f_frsize * statvfs.f_blocks / (1024**3)  # GB
        used_space = total_space - free_space
        
        print(f"   ì´ ê³µê°„: {total_space:.1f}GB")
        print(f"   ì‚¬ìš© ê³µê°„: {used_space:.1f}GB")
        print(f"   ì—¬ìœ  ê³µê°„: {free_space:.1f}GB")
        
        # ëª¨ë¸ ì €ì¥ì— í•„ìš”í•œ ìµœì†Œ ê³µê°„ (ì•½ 15GB)
        min_required = 15.0
        if free_space >= min_required:
            print(f"âœ… ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ (ìµœì†Œ {min_required}GB í•„ìš”)")
            return True
        else:
            print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± (ìµœì†Œ {min_required}GB í•„ìš”, í˜„ì¬ {free_space:.1f}GB)")
            return False
            
    except Exception as e:
        print(f"âŒ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def run_comprehensive_test():
    """ì¢…í•© í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸ” DeepSeek-Coder í•™ìŠµ í™˜ê²½ ì¢…í•© í…ŒìŠ¤íŠ¸ (T4 ìµœì í™”)")
    print("=" * 60)
    print(f"í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    test_results = {}
    
    # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tests = [
        ("GPU í™˜ê²½", test_gpu_environment),
        ("íŒ¨í‚¤ì§€ ì„¤ì¹˜", test_packages),
        ("ë°ì´í„° íŒŒì¼", test_data_file),
        ("ëª¨ë¸ ë¡œë”©", test_model_loading),
        ("ë””ìŠ¤í¬ ê³µê°„", test_disk_space)
    ]
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} í…ŒìŠ¤íŠ¸ {'='*20}")
        try:
            result = test_func()
            test_results[test_name] = result
        except Exception as e:
            print(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            test_results[test_name] = False
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"{test_name:15}: {status}")
        if result:
            passed += 1
    
    print(f"\nì´ {total}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {passed}ê°œ í†µê³¼ ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! í•™ìŠµ í™˜ê²½ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return False

def monitor_spot_instance():
    """ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
    print("\nğŸ” AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
    print("   (Ctrl+Cë¡œ ì¤‘ë‹¨)")
    
    check_interval = 30  # 30ì´ˆë§ˆë‹¤ í™•ì¸
    
    try:
        while True:
            if check_spot_instance_termination():
                print("ğŸš¨ ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì‹ í˜¸ ê°ì§€!")
                print("   í•™ìŠµ ì¤‘ì´ë¼ë©´ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì„ í™•ì¸í•˜ì„¸ìš”.")
                
                # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Python í”„ë¡œì„¸ìŠ¤ í™•ì¸
                try:
                    result = subprocess.run(['pgrep', '-f', 'train.py'], 
                                          capture_output=True, text=True)
                    if result.stdout.strip():
                        print("   ğŸ”„ train.py í”„ë¡œì„¸ìŠ¤ ê°ì§€ë¨ - ìë™ ì €ì¥ ëŒ€ê¸° ì¤‘...")
                except:
                    pass
                
                break
            
            time.sleep(check_interval)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨ë¨")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    if len(sys.argv) > 1 and sys.argv[1] == "--monitor":
        # ëª¨ë‹ˆí„°ë§ ëª¨ë“œ
        monitor_spot_instance()
    else:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        success = run_comprehensive_test()
        
        if success:
            print("\nğŸ’¡ ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•˜ë ¤ë©´:")
            print("   python test_environment.py --monitor")
        
        sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()