# Qwen2.5-Coder-7B-Instruct T4 환경 최적화 설정
model_name: "Qwen/Qwen2.5-7B-Instruct"  # Hugging Face 모델 ID 사용
data_path: "./data/train.jsonl"  # 상대 경로 사용 또는 환경 변수로 설정 가능 (예: ${DATA_PATH})

# T4 최적화 학습 하이퍼파라미터
learning_rate: 2e-4
batch_size: 4  # T4 환경에 맞게 최적화된 배치 크기
grad_acc: 2    # 총 효과적 배치 크기 = 8
max_length: 1024
num_epochs: 3
lr_scheduler: "cosine"
warmup_ratio: 0.1
weight_decay: 0.01

# T4 호환 최적화 설정
fp16: true
bf16: false
gradient_checkpointing: true
torch_dtype: "float16"

# 출력 및 저장 설정
output_dir: "./checkpoints/prompt-finetuned/"  # 상대 경로로 체크포인트 저장 디렉토리 설정
save_steps: 10
eval_steps: 50
logging_steps: 5
save_total_limit: 3
evaluation_strategy: "steps"
save_strategy: "steps"
load_best_model_at_end: true
metric_for_best_model: "eval_loss"

# 스팟 인스턴스 체크포인트 설정
checkpoint_freq: 500  # 500 스텝마다 체크포인트 저장
greater_is_better: false

# 데이터 처리 설정
dataloader_num_workers: 4
remove_unused_columns: false

# SFT 트레이너 설정
use_sft_trainer: true

# QLoRA 설정 (Qwen2.5 권장 설정)
use_lora: true
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# 4비트 양자화 설정 (T4 최적화)
load_in_4bit: true
bnb_4bit_compute_dtype: "float16"
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true

# 추가 설정
max_steps: -1
report_to: null
