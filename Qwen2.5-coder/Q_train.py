#!/usr/bin/env python3
"""
Qwen2.5-coder-7B-instruct ì „ìš© QLoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
- Prompt ëª¨ë“œ ì „ìš© íŒŒì¸íŠœë‹
- Qwen2.5 í† í°í™” í˜•ì‹ ì§€ì›: <|im_start|>role\ncontent<|im_end|>
- T4 GPU ìµœì í™”
- AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ìë™ ê°ì§€ ë° ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
"""

import os
import sys
import json
import yaml
import logging
import argparse
import tempfile
import inspect
import re
from datetime import datetime
from typing import Dict, Any, Optional, List

import torch
import transformers
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments,
    BitsAndBytesConfig,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset, Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# TRL ì„í¬íŠ¸ (ë²„ì „ë³„ ì²˜ë¦¬)
try:
    from trl import SFTTrainer
    TRL_AVAILABLE = True
except ImportError:
    TRL_AVAILABLE = False
    print("âš ï¸ TRL ì—†ìŒ - ê¸°ë³¸ Trainer ì‚¬ìš©")

# ë¡œê¹… ì„¤ì •
# logs ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('logs', exist_ok=True)

# í˜„ì¬ ì‹œê°„ì„ ë¡œê·¸ íŒŒì¼ëª…ì— ì¶”ê°€
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = f"logs/qwen_training_{current_time}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(log_filename, mode='a')
    ]
)
log = logging.getLogger(__name__)

class QwenTrainer:
    """Qwen2.5-coder-7B-instruct prompt ëª¨ë“œìš© QLoRA íŒŒì¸íŠœë‹ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.cfg = self.load_config(config_path)
        self.model = None
        self.tok = None
        self.train_ds = None
        self.eval_ds = None
        
        # T4 í™˜ê²½ ìµœì í™” ì„¤ì • ì ìš©
        self.optimize_for_t4()
        
        log.info(f"ğŸš€ Qwen2.5-coder-7B-instruct íŒŒì¸íŠœë‹ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_int_config(self, key, default=0):
        """configì—ì„œ ê°’ì„ ê°€ì ¸ì™€ intë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        value = self.cfg.get(key, default)
        if isinstance(value, str):
            try:
                value = int(value)
            except ValueError:
                log.warning(f"âš ï¸ ê²½ê³ : {key} ê°’ '{value}'ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
                value = default
        return value
    
    def get_float_config(self, key, default=0.0):
        """configì—ì„œ ê°’ì„ ê°€ì ¸ì™€ floatë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        value = self.cfg.get(key, default)
        if isinstance(value, str):
            try:
                value = float(value)
            except ValueError:
                log.warning(f"âš ï¸ ê²½ê³ : {key} ê°’ '{value}'ì„ ì‹¤ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
                value = default
        return value
        
    def optimize_for_t4(self):
        """T4 GPU í™˜ê²½ì— ë§ëŠ” ìµœì í™” ì„¤ì •"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            compute_cap = torch.cuda.get_device_capability(0)
            
            log.info(f"ğŸ–¥ï¸ GPU: {gpu_name}")
            log.info(f"ğŸ”§ Compute Capability: {compute_cap[0]}.{compute_cap[1]}")
            
            # T4ëŠ” Compute Capability 7.5ì´ë¯€ë¡œ bfloat16 ë¯¸ì§€ì›
            if compute_cap[0] < 8:
                log.info("âš ï¸ bfloat16 ë¯¸ì§€ì› GPU - float16 ì‚¬ìš©")
                self.cfg["bnb_4bit_compute_dtype"] = "float16"
                self.cfg["torch_dtype"] = "float16"
                self.cfg["fp16"] = True
                self.cfg["bf16"] = False
            
            # T4 ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤ (15GB)
            if "T4" in gpu_name:
                log.info("ğŸ”§ T4 í™˜ê²½ ìµœì í™” ì ìš©")
                self.cfg["batch_size"] = 1
                self.cfg["grad_acc"] = max(8, self.get_int_config("grad_acc", 4))
                self.cfg["max_length"] = min(1024, self.get_int_config("max_length", 2048))
    
    @staticmethod
    def default_cfg():
        """T4 í™˜ê²½ ìµœì í™”ëœ ê¸°ë³¸ ì„¤ì •"""
        return {
            # ëª¨ë¸ ì„¤ì • - Qwen ëª¨ë¸ HuggingFace ID
            "model_name": "Qwen/Qwen2.5-coder-7B-instruct",
            "torch_dtype": "float16",
            
            # ì–‘ìí™” ì„¤ì • (T4 ìµœì í™”)
            "load_in_4bit": True,
            "bnb_4bit_compute_dtype": "float16",
            "bnb_4bit_quant_type": "nf4",
            "bnb_4bit_use_double_quant": True,
            
            # LoRA ì„¤ì • - Qwen ëª¨ë¸ì— ë§ëŠ” íƒ€ê²Ÿ ëª¨ë“ˆ ì„¤ì •
            "lora_r": 16,
            "lora_alpha": 32,
            "lora_dropout": 0.1,
            "lora_target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            
            # í•™ìŠµ ì„¤ì • (T4 ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤)
            "batch_size": 1,
            "grad_acc": 8,
            "learning_rate": 2e-4,
            "num_epochs": 3,
            "max_length": 1024,
            "warmup_ratio": 0.1,
            "lr_scheduler": "cosine",
            "weight_decay": 0.01,
            
            # ìµœì í™” ì„¤ì •
            "fp16": True,
            "bf16": False,
            "gradient_checkpointing": True,
            "dataloader_num_workers": 4,
            "remove_unused_columns": False,
            
            # ì €ì¥ ì„¤ì • (save_steps: 10)
            "save_steps": 10,
            "eval_steps": 50,
            "logging_steps": 5,
            "save_total_limit": 5,
            
            # ë°ì´í„° ì„¤ì •
            "data_path": "../data/train.jsonl"
        }
    
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë”©"""
        default_config = self.default_cfg()
        
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                default_config.update(user_config)
                log.info(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë”©: {config_path}")
            except Exception as e:
                log.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        else:
            log.info("ğŸ“ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        
        return default_config
        
    def load_model(self):
        """T4 ìµœì í™”ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©"""
        log.info("ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘")
        log.info("â–¶ï¸  ë² ì´ìŠ¤ ëª¨ë¸ 4-bit ë¡œë“œ")
        
        # ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=self.cfg["load_in_4bit"],
            bnb_4bit_compute_dtype=getattr(torch, self.cfg["bnb_4bit_compute_dtype"]),
            bnb_4bit_quant_type=self.cfg["bnb_4bit_quant_type"],
            bnb_4bit_use_double_quant=self.cfg["bnb_4bit_use_double_quant"],
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        model_path = self.cfg["model_name"]
        
        # HuggingFace ëª¨ë¸ ID í™•ì¸
        is_huggingface_id = '/' not in model_path
        
        log.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {model_path} (HuggingFace ID: {is_huggingface_id})")
        
        try:
            self.tok = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="right",
                local_files_only=False
            )
            log.info(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
        except Exception as e:
            log.error(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì˜¤ë¥˜: {e}")
            raise  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
        
        # íŒ¨ë”© í† í° ì„¤ì • - Qwen ëª¨ë¸ì€ íŒ¨ë”© í† í° ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
        if self.tok.pad_token is None:
            self.tok.pad_token = self.tok.eos_token
            self.tok.pad_token_id = self.tok.eos_token_id
            
        # FIM í† í° í™•ì¸ - Qwenì—ëŠ” FIM í† í°ì´ ìˆëŠ”ì§€ í™•ì¸
        fim_tokens = ['<|fim_prefix|>', '<|fim_middle|>', '<|fim_suffix|>']
        missing_tokens = []
        
        for token in fim_tokens:
            if token not in self.tok.get_vocab():
                missing_tokens.append(token)
        
        if missing_tokens:
            log.warning(f"âš ï¸ ë‹¤ìŒ FIM í† í°ì´ í† í¬ë‚˜ì´ì €ì— ì—†ìŠµë‹ˆë‹¤: {missing_tokens}")
            log.info("FIM í† í°ì„ í† í¬ë‚˜ì´ì €ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
            # í† í° ì¶”ê°€í•˜ê¸°
            self.tok.add_tokens(missing_tokens)
            
        # ëª¨ë¸ ë¡œë”©
        log.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        if torch.cuda.is_available():
            log.info(f"CUDA ë©”ëª¨ë¦¬ ìƒíƒœ (ëª¨ë¸ ë¡œë“œ ì „): ì‚¬ìš© ì¤‘: {torch.cuda.memory_allocated()/1024**3:.2f}GB, ì˜ˆì•½ë¨: {torch.cuda.memory_reserved()/1024**3:.2f}GB")
        
        import time
        start_time = time.time()
        log.info("ëª¨ë¸ ë¡œë”© ì‹œì‘ - ì´ ê³¼ì •ì€ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        
        try:
            # ë¡œë”© ì„¤ì • ì—…ë°ì´íŠ¸ - ë” ëŠë¦¬ì§€ë§Œ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=getattr(torch, self.cfg["torch_dtype"]),
                use_cache=False,
                local_files_only=False,
                low_cpu_mem_usage=True,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
                offload_folder="model_cache"  # ë¡œì»¬ ì˜¤í”„ë¡œë“œ í´ë” ì§€ì •
            )
            end_time = time.time()
            log.info(f"ëª¨ë¸ ë¡œë”© ì„±ê³µ! ì†Œìš” ì‹œê°„: {(end_time-start_time):.2f}ì´ˆ")
            
            if torch.cuda.is_available():
                log.info(f"CUDA ë©”ëª¨ë¦¬ ìƒíƒœ (ëª¨ë¸ ë¡œë“œ í›„): ì‚¬ìš© ì¤‘: {torch.cuda.memory_allocated()/1024**3:.2f}GB, ì˜ˆì•½ë¨: {torch.cuda.memory_reserved()/1024**3:.2f}GB")
                
        except Exception as e:
            log.error(f"ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}")
            import traceback
            log.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            raise  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
            
        # í† í¬ë‚˜ì´ì €ì— ìƒˆ í† í°ì„ ì¶”ê°€í–ˆë‹¤ë©´ ëª¨ë¸ì˜ ì„ë² ë”© í¬ê¸° ì¡°ì •
        if missing_tokens:
            log.info(f"ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì • (FIM í† í° ì¶”ê°€)")
            self.model.resize_token_embeddings(len(self.tok))
        
        # k-bit í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„
        self.model = prepare_model_for_kbit_training(
            self.model, 
            use_gradient_checkpointing=self.cfg["gradient_checkpointing"]
        )
        log.info("âœ… ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ìœ¼ë¡œ k-bit í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ.")
        
        # LoRA ì„¤ì • ë° ì ìš©
        lora_config = LoraConfig(
            r=self.cfg["lora_r"],
            lora_alpha=self.cfg["lora_alpha"],
            target_modules=self.cfg["lora_target_modules"],
            lora_dropout=self.cfg["lora_dropout"],
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        log.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def load_data(self):
        """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"""
        log.info("ğŸ”„ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì‹œì‘")
        log.info(f"â–¶ï¸  ë°ì´í„° ë¡œë”©: {self.cfg['data_path']}")
        
        # JSONL íŒŒì¼ ë¡œë”©
        dataset = load_dataset("json", data_files=self.cfg["data_path"], split="train")
        
        log.info(f"ğŸ“Š ë°ì´í„°ì…‹ ì»¬ëŸ¼: {dataset.column_names}")
        log.info(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ ìƒ˜í”Œ")
        
        # ë°ì´í„° í˜•ì‹ì„ ê°ì§€í•˜ê³  ì ì ˆí•œ ë³€í™˜ í•¨ìˆ˜ ì„ íƒ
        def detect_format(example):
            """ë°ì´í„° í˜•ì‹ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜ - í•„ìˆ˜ í˜•ì‹ë§Œ ì§€ì›"""
            if "system" in example and "user" in example and "assistant" in example:
                return "system_user_assistant"  # ì‹œìŠ¤í…œ/ì‚¬ìš©ì/ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ í¬ë§·
            elif "meta" in example and "user" in example and "assistant" in example:
                return "meta_user_assistant"  # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì‚¬ìš©ì/ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ í˜•ì‹
            elif "prompt" in example and "completion" in example:
                return "prompt_completion"  # í‘œì¤€ í”„ë¡¬í”„íŠ¸-ì™„ì„± í˜•ì‹ (ê¸°ë³¸ í˜•ì‹)
            else:
                # ë°ì´í„° êµ¬ì¡° ë¡œê¹… - ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹
                log.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: {list(example.keys())}")
                return "unknown"
        
        # ìƒ˜í”Œì„ ê²€ì‚¬í•˜ì—¬ ë°ì´í„° í˜•ì‹ ê°ì§€
        sample = dataset[0]
        data_format = detect_format(sample)
        log.info(f"âœ… ê°ì§€ëœ ë°ì´í„° í˜•ì‹: {data_format}")
        
        # í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
        def tokenize_function(examples):
            # ë°ì´í„° í˜•ì‹ì— ë”°ë¼ Qwen í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            texts = []
            
            # ìƒ˜í”Œ ìˆ˜ ê²°ì •
            if "user" in examples and "assistant" in examples:
                sample_count = len(examples["user"])  # system, user, assistantëŠ” ê°™ì€ ê¸¸ì´ë¥¼ ê°€ì •
            elif "prompt" in examples and "completion" in examples:
                sample_count = len(examples["prompt"])
            else:
                log.error("âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹")
                raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤.")
            
            # Qwen2.5 í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³µí†µ í•¨ìˆ˜
            def format_qwen_message(system_msg, user_msg, assistant_msg):
                text = f"<|im_start|>system\n{system_msg}<|im_end|>\n"
                text += f"<|im_start|>user\n{user_msg}<|im_end|>\n"
                text += f"<|im_start|>assistant\n{assistant_msg}<|im_end|>"
                return text
                
            for i in range(sample_count):
                # ë°ì´í„° í˜•ì‹ë³„ ì²˜ë¦¬
                if data_format == "system_user_assistant":
                    system = examples["system"][i].strip()
                    user = examples["user"][i].strip()
                    assistant = examples["assistant"][i].strip()
                    
                    # FIM í† í° ì²˜ë¦¬ (Fill-in-the-Middle í˜•ì‹ ì§€ì›)
                    if "<|fim_prefix|>" in user and "<|fim_middle|>" in user and "<|fim_suffix|>" in user:
                        log.debug("ğŸ” FIM í˜•ì‹ ê°ì§€ë¨")
                    
                    # ê³µí†µ í•¨ìˆ˜ë¥¼ í†µí•œ ë³€í™˜
                    texts.append(format_qwen_message(system, user, assistant))
                
                elif data_format == "meta_user_assistant":
                    # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì‚¬ìš©ì/ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ í˜•ì‹
                    meta = examples["meta"][i]
                    user = examples["user"][i].strip()
                    assistant = examples["assistant"][i].strip()
                    
                    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ì¶œ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
                    if "persona_name" in meta and "scenario" in meta:
                        system_msg = f"You are {meta['persona_name']} who helps users with {meta['scenario']}"
                    else:
                        system_msg = "You are a helpful coding assistant."
                    
                    # ê³µí†µ í•¨ìˆ˜ë¥¼ í†µí•œ ë³€í™˜
                    texts.append(format_qwen_message(system_msg, user, assistant))
                
                elif data_format == "prompt_completion":
                    # í”„ë¡¬í”„íŠ¸-ì™„ì„± í˜•ì‹ì„ Qwen ë©”ì„¸ì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    prompt = examples["prompt"][i].strip()
                    completion = examples["completion"][i].strip()
                    
                    # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì‚¬ìš©
                    system_msg = "You are a helpful Python coding assistant."
                    
                    # FIM í† í° ì²˜ë¦¬ (prompt ëª¨ë“œì—ì„œë§Œ í•„ìš”)
                    if "<|fim_prefix|>" in prompt and "<|fim_middle|>" in prompt and "<|fim_suffix|>" in prompt:
                        log.debug("ğŸ” FIM í˜•ì‹ ê°ì§€ë¨")
                    
                    # ê³µí†µ í•¨ìˆ˜ë¥¼ í†µí•œ ë³€í™˜
                    texts.append(format_qwen_message(system_msg, prompt, completion))
                
                else:
                    log.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: {data_format}")
                    # ëŒ€ì²´ ì‚¬ìš©í•  ì„ì‹œ ë°ì´í„°
                    texts.append("<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nHello<|im_end|>\n<|im_start|>assistant\nHi there!<|im_end|>")
            # í† í¬ë‚˜ì´ì§•
            model_inputs = self.tok(
                texts,
                truncation=True,
                padding=False,
                max_length=self.cfg["max_length"],
                return_tensors=None
            )
            
            # labels = input_ids (causal LM)
            model_inputs["labels"] = model_inputs["input_ids"].copy()
            
            return model_inputs
        
        # ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•
        dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
            desc="í† í¬ë‚˜ì´ì§• ì¤‘"
        )
        
        log.info("âœ… ë°ì´í„° í† í¬ë‚˜ì´ì§• ì™„ë£Œ")
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
        self.train_ds = split_dataset["train"]
        self.eval_ds = split_dataset["test"]
        
        log.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(self.train_ds)}ê°œ")
        log.info(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(self.eval_ds)}ê°œ")
        
    def t_args(self) -> TrainingArguments:
        """í•™ìŠµ ì¸ì ìƒì„± - prompt ëª¨ë“œ ì „ìš©"""
        # í•™ìŠµ ê²½ë¡œ ì„¤ì • - ì²´í¬í¬ì¸íŠ¸ì™€ ì¶œë ¥ìš©
        output_dir = "../scripts/checkpoints"
        
        # ìµœì¢… ëª¨ë¸ ê²½ë¡œ
        self.final_model_dir = "../models/prompt-finetuned"
        
        # ìµœì†Œí•œì˜ ì•ˆì „í•œ ì¸ìë§Œ ì‚¬ìš© (íƒ€ì… ë³€í™˜ ì¶”ê°€)
        return TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=self.get_int_config("batch_size", 1),
            per_device_eval_batch_size=self.get_int_config("batch_size", 1),
            gradient_accumulation_steps=self.get_int_config("grad_acc", 8),
            learning_rate=self.get_float_config("learning_rate", 2e-4),
            num_train_epochs=self.get_float_config("num_epochs", 3),
            warmup_ratio=self.get_float_config("warmup_ratio", 0.1),
            lr_scheduler_type=self.cfg["lr_scheduler"],
            weight_decay=self.get_float_config("weight_decay", 0.01),
            fp16=self.cfg["fp16"],
            bf16=self.cfg["bf16"],
            gradient_checkpointing=self.cfg["gradient_checkpointing"],
            logging_steps=self.get_int_config("logging_steps", 5),
            evaluation_strategy="steps",
            eval_steps=self.get_int_config("eval_steps", 50),
            save_strategy="steps",
            save_steps=self.get_int_config("save_steps", 10),
            save_total_limit=self.get_int_config("save_total_limit", 5),
            remove_unused_columns=self.cfg["remove_unused_columns"],
            dataloader_num_workers=self.get_int_config("dataloader_num_workers", 4),
            report_to="none",  # ë¹ ë¥¸ ì‹œì‘ì„ ìœ„í•´ ë³´ê³  ê¸°ëŠ¥ ë¹„í™œì„±í™”
            # ì¶”ê°€ í•„ìš” ì„¤ì •
            push_to_hub=False,
            group_by_length=True  # ë¹„ìŠ·í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ í•¨ê»˜ ë°°ì¹˜í•˜ì—¬ íŒ¨ë”© ìµœì†Œí™”
        )

    def find_latest_checkpoint(self, checkpoint_dir):
        """ê°€ì¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
        
        Args:
            checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
            
        Returns:
            str: ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë˜ëŠ” None
        """
        if not os.path.exists(checkpoint_dir):
            log.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {checkpoint_dir}")
            return None
        
        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ë‚´ì—ì„œ checkpoint-* íŒ¨í„´ì„ ì°¾ìŒ
        checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith("checkpoint-")]
        if not checkpoints:
            log.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_dir}")
            return None
            
        # ì²´í¬í¬ì¸íŠ¸ ë²ˆí˜¸ë¡œ ì •ë ¬
        checkpoints = sorted(checkpoints, key=lambda x: int(x.split("-")[1]))
        latest_checkpoint = os.path.join(checkpoint_dir, checkpoints[-1])
        
        log.info(f"ğŸ” ê°€ì¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {latest_checkpoint}")
        return latest_checkpoint

    def train(self):
        """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
        log.info("ğŸš€ í•™ìŠµ ì‹œì‘")

        # ëª¨ë¸, ë°ì´í„° ë¡œë“œ
        self.load_model()
        self.load_data()

        # í›ˆë ¨ ì¸ì ìƒì„±
        args = self.t_args()

        # ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tok,
            mlm=False  # CLM ì‚¬ìš©
        )

        # SpotInterruptionHandler ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ê°ì§€)
        spot_handler = SpotInterruptionHandler()
        spot_handler.start_monitoring()
        log.info("âœ… AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ê°ì§€ í™œì„±í™”")

        try:
            # TRL ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ Trainer ì„ íƒ
            if TRL_AVAILABLE:
                log.info("âœ… TRL SFTTrainer ì‚¬ìš©")
                trainer = SFTTrainer(
                    model=self.model,
                    args=args,
                    train_dataset=self.train_ds,
                    eval_dataset=self.eval_ds,
                    tokenizer=self.tok,
                    data_collator=data_collator,
                    max_seq_length=self.cfg["max_length"]
                )
            else:
                log.info("âœ… ê¸°ë³¸ Trainer ì‚¬ìš©")
                trainer = Trainer(
                    model=self.model,
                    args=args,
                    train_dataset=self.train_ds,
                    eval_dataset=self.eval_ds,
                    tokenizer=self.tok,
                    data_collator=data_collator
                )

            # ì²´í¬í¬ì¸íŠ¸ ìë™ ê°ì§€ ë° í•™ìŠµ ì¬ê°œ ì„¤ì •
            checkpoint_path = None
            
            # í™˜ê²½ ë³€ìˆ˜ì—ì„œ auto ì„¤ì • ê°ì§€
            resume_setting = os.environ.get('RESUME_CHECKPOINT', '').strip().lower()
            auto_resume = (resume_setting == 'auto')
            
            if auto_resume:
                log.info("ğŸ” ì²´í¬í¬ì¸íŠ¸ ìë™ ê°ì§€ ëª¨ë“œ í™œì„±í™”")
                checkpoint_path = self.find_latest_checkpoint(args.output_dir)
                if checkpoint_path:
                    log.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ìë™ ê°ì§€ ì™„ë£Œ: {checkpoint_path}")
                else:
                    log.info("ğŸ’¡ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ì–´ì„œ ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            elif args.resume_from_checkpoint is not None:
                # ëª…ì‹œì ìœ¼ë¡œ ì§€ì •ëœ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
                log.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ: {args.resume_from_checkpoint}")
                checkpoint_path = args.resume_from_checkpoint

            # í•™ìŠµ ì‹œì‘
            log.info(f"ğŸš€ í•™ìŠµ ì‹œì‘ (resume_from_checkpoint={checkpoint_path})")
            train_result = trainer.train(resume_from_checkpoint=checkpoint_path)
            
            # ìµœì¢… ëª¨ë¸ ì €ì¥
            log.info(f"âœ… í•™ìŠµ ì™„ë£Œ, ì €ì¥ ì¤‘... ì¶œë ¥ ê²½ë¡œ: {self.final_model_dir}")
            trainer.save_state()
            
            # ìµœì¢… ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.final_model_dir, exist_ok=True)
            
            # ëª¨ë¸ ì €ì¥ (peft í˜•íƒœë¡œ)
            self.model.save_pretrained(self.final_model_dir)
            self.tok.save_pretrained(self.final_model_dir)
            
            # config.json íŒŒì¼ì— í•™ìŠµ ì •ë³´ ì¶”ê°€
            config_path = os.path.join(self.final_model_dir, "config.json")
            if os.path.exists(config_path):
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config_data = json.load(f)
                    
                    # í•™ìŠµ ì •ë³´ ì¶”ê°€
                    training_info = {
                        "training_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "model_name": self.cfg["model_name"],
                        "data_path": self.cfg["data_path"],
                        "training_loss": float(train_result.training_loss),
                        "training_runtime": float(train_result.metrics['train_runtime']),
                        "training_samples": len(self.train_ds),
                    }
                    
                    config_data["training_info"] = training_info
                    
                    with open(config_path, 'w', encoding='utf-8') as f:
                        json.dump(config_data, f, indent=2, ensure_ascii=False)
                        
                    log.info("âœ… í•™ìŠµ ì •ë³´ê°€ config.jsonì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    log.warning(f"âš ï¸ config.json ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            
            # í•™ìŠµ ê²°ê³¼ ë¡œê·¸
            log.info("âœ… í•™ìŠµ ì™„ë£Œ!")
            log.info(f"âœ… ìµœì¢… loss: {train_result.training_loss}")
            log.info(f"âœ… ì´ í•™ìŠµ ì‹œê°„: {train_result.metrics['train_runtime']}ì´ˆ")
            log.info(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {self.final_model_dir}")
            log.info(f"ğŸ“¦ HuggingFace ëª¨ë¸ ID: {self.cfg['model_name']}")
            log.info(f"ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {self.cfg['data_path']}")
            log.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {args.output_dir}")
            log.info("ğŸ‘ ìƒˆë¡œìš´ Qwen2.5-Coder-7B í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            log.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            log.error(traceback.format_exc())
            raise
        finally:
            # ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            spot_handler.stop_monitoring()


class SpotInterruptionHandler:
    """AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ê°ì§€ ë° ì²˜ë¦¬ í´ë˜ìŠ¤
    
    ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ê°ì§€ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í›„ ì¢…ë£Œí•˜ëŠ” ê¸°ëŠ¥ ì œê³µ
    """
    
    def __init__(self):
        self.monitor_thread = None
        self.should_stop = False
        self.meta_endpoint = "http://169.254.169.254/latest/meta-data/spot/instance-action"
        self.check_interval = 5  # 5ì´ˆë§ˆë‹¤ ì²´í¬
        
        # SIGTERM í•¸ë“¤ëŸ¬ ë“±ë¡
        import signal
        self.original_handler = signal.getsignal(signal.SIGTERM)
        signal.signal(signal.SIGTERM, self._handle_sigterm)
    
    def _handle_sigterm(self, signum, frame):
        """ì‹œê·¸í„´ ì‹ í˜¸ ì²˜ë¦¬"""
        log.warning("ğŸš¨ SIGTERM ì‹ í˜¸ ê°ì§€! ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        
        # ì›ë˜ í•¸ë“¤ëŸ¬ í˜¸ì¶œ
        if callable(self.original_handler):
            self.original_handler(signum, frame)
    
    def check_interruption(self):
        """ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì˜ˆê³  ê°ì§€"""
        try:
            import requests
            # AWS ë©”íƒ€ë°ì´í„° ì—”ë“œí¬ì¸íŠ¸ ìš”ì²­
            response = requests.get(self.meta_endpoint, timeout=0.5)
            
            if response.status_code == 200:
                log.warning("âš ï¸ AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ê°ì§€!")
                
                # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì— SIGTERM ì‹ í˜¸ ë³´ë‚´ê¸°
                import signal
                os.kill(os.getpid(), signal.SIGTERM)
                return True
                
            return False
        except:
            return False
    
    def monitor(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ì¤‘ë‹¨ ê°ì§€"""
        log.info("ğŸ” AWS ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        import time
        
        while not self.should_stop:
            if self.check_interruption():
                break
                
            # 5ì´ˆë§ˆë‹¤ ì²´í¬
            time.sleep(self.check_interval)
                
        log.info("ğŸ›‘ ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        import threading
        self.should_stop = False
        
        # AWS í™˜ê²½ì¸ì§€ ì²´í¬
        try:
            import requests
            requests.get("http://169.254.169.254/", timeout=0.2)
            is_aws = True
        except:
            is_aws = False
        
        if not is_aws:
            log.warning("âš ï¸ AWS í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ëª¨ë‹ˆí„°ë§ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            return
        
        self.monitor_thread = threading.Thread(target=self.monitor, daemon=True)
        self.monitor_thread.start()
        log.info("âœ… ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if self.monitor_thread is None:
            return
            
        self.should_stop = True
        try:
            self.monitor_thread.join(timeout=1.0)  # ìµœëŒ€ 1ì´ˆ ëŒ€ê¸°
        except:
            pass
        
        self.monitor_thread = None
        log.info("âœ… ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")


def main():
    """ë©”ì¸ í•¨ìˆ˜ - Qwen2.5-coder-7B-instruct prompt ëª¨ë“œ íŒŒì¸íŠœë‹ ì‹œì‘"""
    parser = argparse.ArgumentParser(description="Qwen2.5-coder-7B-instruct Prompt ëª¨ë“œ QLoRA íŒŒì¸íŠœë‹")
    parser.add_argument(
        "--config", 
        type=str, 
        default="config.yaml", 
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )
    args = parser.parse_args()
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    trainer = QwenTrainer(config_path=args.config)
    trainer.train()


if __name__ == "__main__":
    main()